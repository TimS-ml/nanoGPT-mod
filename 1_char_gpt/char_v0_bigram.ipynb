{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "v0: \n",
        "- Bigram, shakespeare char-based GPT\n",
        "\n",
        "Source:\n",
        "- https://youtu.be/kCc8FmEb1nY\n",
        "- https://github.com/karpathy/ng-video-lecture/blob/master/bigram.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MF5SMVST_0Qj"
      },
      "outputs": [],
      "source": [
        "import os; os.chdir('..')\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "from utils import *\n",
        "from boring_utils.utils import init_graph, set_seed, get_device, cprint, tprint\n",
        "from utils import add_to_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "init_graph()\n",
        "set_seed(1337)\n",
        "device = get_device()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JgFwbyBK_0Qj"
      },
      "source": [
        "# Encode and Decode Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CsiIKfLP_0Qk",
        "outputId": "26f46cf7-a8ae-4dc2-a0bb-a03da944572d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of dataset in characters: 1,115,394\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "stoi: {'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
            "itos: {0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
          ]
        }
      ],
      "source": [
        "input_file_path = './data/shakespeare_char/input.txt'\n",
        "\n",
        "with open(input_file_path, 'r') as f:\n",
        "    data = f.read()\n",
        "print(f\"length of dataset in characters: {len(data):,}\")\n",
        "\n",
        "# get all the unique characters that occur in this text\n",
        "chars = sorted(list(set(data)))\n",
        "vocab_size = len(chars)\n",
        "print(\"all the unique characters:\", ''.join(chars))\n",
        "print(f\"vocab size: {vocab_size:,}\")\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "print(\"stoi:\", stoi)\n",
        "print(\"itos:\", itos)\n",
        "\n",
        "def encode(s):\n",
        "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "\n",
        "def decode(l):\n",
        "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_decode(net, max_new_tokens=100):\n",
        "    gen = net.generate(\n",
        "            torch.zeros((1, 1), dtype=torch.long),\n",
        "            max_new_tokens=max_new_tokens\n",
        "        )\n",
        "    print(gen)\n",
        "    print(decode(gen[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "f8k_FXKB_0Qk",
        "outputId": "51776729-cc55-4264-e1e0-43ec6788e520"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1003854]) torch.int64\n",
            "torch.Size([111540]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "train_bin_path = './data/shakespeare_char/train.bin'\n",
        "val_bin_path = './data/shakespeare_char/val.bin'\n",
        "\n",
        "# train_tensor = torch.tensor(encode(data), dtype=torch.long) # convert to tensor\n",
        "\n",
        "# torch.long is just an alias for torch.int64\n",
        "# load the binary data\n",
        "train_data = np.fromfile(train_bin_path, dtype=np.uint16)\n",
        "val_data = np.fromfile(val_bin_path, dtype=np.uint16)\n",
        "\n",
        "# convert to pytorch tensors\n",
        "train_tensor = torch.from_numpy(train_data.astype(np.int64))\n",
        "val_tensor = torch.from_numpy(val_data.astype(np.int64))\n",
        "\n",
        "print(train_tensor.shape, train_tensor.dtype)\n",
        "print(val_tensor.shape, val_tensor.dtype)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preview"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HtK4TkbA_0Qk"
      },
      "source": [
        "## batch_size = 1\n",
        "\n",
        "The sequence length will be incremented by 1 each time `t` until the block_size (context window size)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EnV--qOT_0Ql",
        "outputId": "76743126-2036-487f-ea85-dbe994d8a2f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[93m<module> -> train_data[:block_size]:\u001b[0m\n",
            "array([18, 47, 56, 57, 58,  1, 15, 47], dtype=uint16)\n",
            "\n",
            "==================== <module> -> Decoding each time step ====================\n",
            "t=0, context=[18], decode_c=F, target=47, decode_t=i\n",
            "t=1, context=[18 47], decode_c=Fi, target=56, decode_t=r\n",
            "t=2, context=[18 47 56], decode_c=Fir, target=57, decode_t=s\n",
            "t=3, context=[18 47 56 57], decode_c=Firs, target=58, decode_t=t\n",
            "t=4, context=[18 47 56 57 58], decode_c=First, target=1, decode_t= \n",
            "t=5, context=[18 47 56 57 58  1], decode_c=First , target=15, decode_t=C\n",
            "t=6, context=[18 47 56 57 58  1 15], decode_c=First C, target=47, decode_t=i\n",
            "t=7, context=[18 47 56 57 58  1 15 47], decode_c=First Ci, target=58, decode_t=t\n"
          ]
        }
      ],
      "source": [
        "block_size = 8\n",
        "cprint(train_data[:block_size])\n",
        "\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]  # 1 char ahead\n",
        "\n",
        "tprint('Decoding each time step')\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"t={t}, context={context}, decode_c={decode(context)}, target={target}, decode_t={decode([target])}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "izvPt1qD_0Ql"
      },
      "source": [
        "## batch_size != 1\n",
        "\n",
        "check batch.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "apeHrXQi_0Ql",
        "outputId": "7e5ca2df-fd00-4c9c-ea8f-9c5eb4d3052e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================== <module> -> Batch Preview (no random sampling) ====================\n",
            "\u001b[93mget_batch -> ix:\u001b[0m\n",
            "tensor([0, 1, 2, 3])\n",
            "\n",
            "==================== <module> -> xb ====================\n",
            "torch.Size([4, 8]) torch.int64\n",
            "tensor([[18, 47, 56, 57, 58,  1, 15, 47],\n",
            "        [47, 56, 57, 58,  1, 15, 47, 58],\n",
            "        [56, 57, 58,  1, 15, 47, 58, 47],\n",
            "        [57, 58,  1, 15, 47, 58, 47, 64]])\n",
            "\n",
            "==================== <module> -> yb ====================\n",
            "torch.Size([4, 8]) torch.int64\n",
            "tensor([[47, 56, 57, 58,  1, 15, 47, 58],\n",
            "        [56, 57, 58,  1, 15, 47, 58, 47],\n",
            "        [57, 58,  1, 15, 47, 58, 47, 64],\n",
            "        [58,  1, 15, 47, 58, 47, 64, 43]])\n"
          ]
        }
      ],
      "source": [
        "batch_size = 4  # how many independent sequences to train on in parallel\n",
        "block_size = 8  # what is the maximum concatenated length for predictions\n",
        "\n",
        "def get_batch(split, random_sample=True):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "\n",
        "    if not random_sample:\n",
        "        ix = torch.arange(batch_size)\n",
        "    else:\n",
        "        # NOTE: `len(data) - block_size` is the maximum index\n",
        "        ix = torch.randint(\n",
        "            len(data) - block_size, \n",
        "            (batch_size,)  # we sample \"batch_size\" random indices\n",
        "        )\n",
        "    cprint(ix)\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    return x, y\n",
        "\n",
        "\n",
        "tprint('Batch Preview (no random sampling)')\n",
        "xb, yb = get_batch('train', random_sample=False)\n",
        "\n",
        "tprint('xb')\n",
        "print(xb.shape, xb.dtype)\n",
        "print(xb)\n",
        "\n",
        "tprint('yb')\n",
        "print(yb.shape, yb.dtype)\n",
        "print(yb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jH4gDgnB_0Ql",
        "outputId": "b929c539-4097-4691-eaf4-d909d23a3dcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================== <module> -> Batch decoding each time step ====================\n",
            "\n",
            "-------------------- <module> -> batch 0 --------------------\n",
            "b=0, t=0, context=tensor([18]), target=47\n",
            "decode_c=F, decode_t=i\n",
            "b=0, t=1, context=tensor([18, 47]), target=56\n",
            "decode_c=Fi, decode_t=r\n",
            "b=0, t=2, context=tensor([18, 47, 56]), target=57\n",
            "decode_c=Fir, decode_t=s\n",
            "b=0, t=3, context=tensor([18, 47, 56, 57]), target=58\n",
            "decode_c=Firs, decode_t=t\n",
            "b=0, t=4, context=tensor([18, 47, 56, 57, 58]), target=1\n",
            "decode_c=First, decode_t= \n",
            "b=0, t=5, context=tensor([18, 47, 56, 57, 58,  1]), target=15\n",
            "decode_c=First , decode_t=C\n",
            "b=0, t=6, context=tensor([18, 47, 56, 57, 58,  1, 15]), target=47\n",
            "decode_c=First C, decode_t=i\n",
            "b=0, t=7, context=tensor([18, 47, 56, 57, 58,  1, 15, 47]), target=58\n",
            "decode_c=First Ci, decode_t=t\n",
            "\n",
            "\n",
            "-------------------- <module> -> batch 1 --------------------\n",
            "b=1, t=0, context=tensor([47]), target=56\n",
            "decode_c=i, decode_t=r\n",
            "b=1, t=1, context=tensor([47, 56]), target=57\n",
            "decode_c=ir, decode_t=s\n",
            "b=1, t=2, context=tensor([47, 56, 57]), target=58\n",
            "decode_c=irs, decode_t=t\n",
            "b=1, t=3, context=tensor([47, 56, 57, 58]), target=1\n",
            "decode_c=irst, decode_t= \n",
            "b=1, t=4, context=tensor([47, 56, 57, 58,  1]), target=15\n",
            "decode_c=irst , decode_t=C\n",
            "b=1, t=5, context=tensor([47, 56, 57, 58,  1, 15]), target=47\n",
            "decode_c=irst C, decode_t=i\n",
            "b=1, t=6, context=tensor([47, 56, 57, 58,  1, 15, 47]), target=58\n",
            "decode_c=irst Ci, decode_t=t\n",
            "b=1, t=7, context=tensor([47, 56, 57, 58,  1, 15, 47, 58]), target=47\n",
            "decode_c=irst Cit, decode_t=i\n",
            "\n",
            "\n",
            "-------------------- <module> -> batch 2 --------------------\n",
            "b=2, t=0, context=tensor([56]), target=57\n",
            "decode_c=r, decode_t=s\n",
            "b=2, t=1, context=tensor([56, 57]), target=58\n",
            "decode_c=rs, decode_t=t\n",
            "b=2, t=2, context=tensor([56, 57, 58]), target=1\n",
            "decode_c=rst, decode_t= \n",
            "b=2, t=3, context=tensor([56, 57, 58,  1]), target=15\n",
            "decode_c=rst , decode_t=C\n",
            "b=2, t=4, context=tensor([56, 57, 58,  1, 15]), target=47\n",
            "decode_c=rst C, decode_t=i\n",
            "b=2, t=5, context=tensor([56, 57, 58,  1, 15, 47]), target=58\n",
            "decode_c=rst Ci, decode_t=t\n",
            "b=2, t=6, context=tensor([56, 57, 58,  1, 15, 47, 58]), target=47\n",
            "decode_c=rst Cit, decode_t=i\n",
            "b=2, t=7, context=tensor([56, 57, 58,  1, 15, 47, 58, 47]), target=64\n",
            "decode_c=rst Citi, decode_t=z\n",
            "\n",
            "\n",
            "-------------------- <module> -> batch 3 --------------------\n",
            "b=3, t=0, context=tensor([57]), target=58\n",
            "decode_c=s, decode_t=t\n",
            "b=3, t=1, context=tensor([57, 58]), target=1\n",
            "decode_c=st, decode_t= \n",
            "b=3, t=2, context=tensor([57, 58,  1]), target=15\n",
            "decode_c=st , decode_t=C\n",
            "b=3, t=3, context=tensor([57, 58,  1, 15]), target=47\n",
            "decode_c=st C, decode_t=i\n",
            "b=3, t=4, context=tensor([57, 58,  1, 15, 47]), target=58\n",
            "decode_c=st Ci, decode_t=t\n",
            "b=3, t=5, context=tensor([57, 58,  1, 15, 47, 58]), target=47\n",
            "decode_c=st Cit, decode_t=i\n",
            "b=3, t=6, context=tensor([57, 58,  1, 15, 47, 58, 47]), target=64\n",
            "decode_c=st Citi, decode_t=z\n",
            "b=3, t=7, context=tensor([57, 58,  1, 15, 47, 58, 47, 64]), target=43\n",
            "decode_c=st Citiz, decode_t=e\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tprint('Batch decoding each time step')\n",
        "for b in range(batch_size):\n",
        "    tprint(f'batch {b}', sep='-')\n",
        "    for t in range(block_size):\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b, t]\n",
        "        print(f\"b={b}, t={t}, context={context}, target={target}\")\n",
        "        print(f\"decode_c={decode(context.tolist())}, decode_t={decode([target.tolist()])}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xrAZAPqy_0Ql"
      },
      "source": [
        "# NN - BigramLanguageModel\n",
        "\n",
        "Note: unlike `F.one_hot`, `nn.Embedding` contains learnable parameters (vocab_size, embedding_dim). And can also lead to better performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This design choice reflects the fact that this is a bigram model, which is a simple type of language model that only considers the immediately preceding token when predicting the next token. Because it's a bigram model, it doesn't need to consider more complex patterns over longer sequences of tokens, which is what you'd typically use an RNN or similar model for. Instead, it can use the embedding of the current token to predict the next token directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bx1iQWA0_0Ql"
      },
      "outputs": [],
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        embedding_dim = vocab_size\n",
        "        # embedding_dim = 128\n",
        "        # each token is represented by a one-hot vector\n",
        "        # directly reads off the logits for the next token from the embedding table\n",
        "        # for example: 24 will reads off the 24th column of the embedding table\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is (batch_size, block_size)\n",
        "        logits = self.embedding(idx)  # B, T, C: (batch_size, block_size, embedding_dim)\n",
        "\n",
        "        if targets == None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)  # (batch_size * block_size, embedding_dim)\n",
        "            targets = targets.view(-1)  # (batch_size * block_size)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kLUUvfRY_0Ql"
      },
      "source": [
        "`-ln(1/65)` is essentially the baseline or the \"no information\" rate. Any model should ideally perform better than this.\n",
        "\n",
        "A lower loss means that the model's predictions are better than random guessing. A higher loss means the model's predictions are worse than random guessing. \n",
        "\n",
        "Cross entropy loss = `-sum(y_i * log(p_i))` for all classes `i`. \n",
        "Where `y_i` is the true label (1 for the correct class and 0 for all other classes), and `p_i` is the predicted probability for class `i` (random selection `p_i` = 1/65).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9MCDiZV_0Ql",
        "outputId": "4850eafc-e30c-428c-cd52-4f270b01424d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 65]) torch.float32\n",
            "tensor(4.2793, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# in prepare.py, we have:\n",
        "# chars = sorted(list(set(data)))\n",
        "# vocab_size = len(chars)\n",
        "\n",
        "m = BigramLanguageModel(65)\n",
        "m.to(device)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape, logits.dtype)\n",
        "print(loss)  # so currently this is worse than random guessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yN6eFPK__0Qm"
      },
      "outputs": [],
      "source": [
        "@add_to_class(BigramLanguageModel)\n",
        "def generate(self, idx, max_new_tokens):\n",
        "    # idx is (batch_size, block_size)\n",
        "    for _ in range(max_new_tokens):\n",
        "        # get the predictions\n",
        "        # logits, _ = self.forward(idx, None)\n",
        "        logits, _ = self(idx)\n",
        "\n",
        "        # focus only the last time stemp\n",
        "        logits = logits[:, -1, :]  # (batch_size, embedding_dim)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # sample from distrubution\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # append sampled idx to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, T + 1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2B465umQ_0Qm",
        "outputId": "32fed72c-3351-404a-980e-8b9bf6fe8bc3"
      },
      "outputs": [],
      "source": [
        "test_decode(m)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uPzIdh_mACRz"
      },
      "source": [
        "# Training the NN\n",
        "\n",
        "[torch.optim.Optimizer.zero_grad â€” PyTorch 2.0 documentation](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8UVyGhL_98h"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWJ7yFaDA2FI"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "loss_list = []\n",
        "\n",
        "for steps in range(20000):\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    # interesting...\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if steps % 200 == 0:\n",
        "        loss_list.append(loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmFIdFlkB-p8"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(20, 20))\n",
        "plt.plot(np.arange(1, len(loss_list) + 1), loss_list, label=\"Train loss\")\n",
        "plt.xlabel(\"Loss\")\n",
        "plt.ylabel(\"Epochs\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_decode(m, 200)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
