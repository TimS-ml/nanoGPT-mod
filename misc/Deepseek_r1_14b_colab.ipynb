{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DeepSeek R1 14B Colab Inference\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/TimS-ml/nanoGPT-mod/blob/master/misc/Deepseek_r1_14b_colab.ipynb)\n",
        "\n",
        "[![Open In GitHub](https://img.shields.io/badge/Open%20In%20GitHub-black?style=flat&logo=github)](https://github.com/TimS-ml/nanoGPT-mod/blob/master/misc/Deepseek_r1_14b_colab.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Normally using pip install unsloth is enough\n",
        "\n",
        "# Temporarily as of Jan 31st 2025, Colab has some issues with Pytorch\n",
        "# Using pip install unsloth will take 3 minutes, whilst the below takes <1 minute:\n",
        "!pip install --no-deps bitsandbytes accelerate xformers peft trl triton\n",
        "!pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHXNCLXDzweS",
        "outputId": "231fefa6-0301-439e-93fe-7bad446061f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q unsloth\n",
        "!pip uninstall -q unsloth -y && pip install -q --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX9DXfpjkXfQ"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "# model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\"\n",
        "model_name = \"unsloth/DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit\"\n",
        "\n",
        "max_seq_length = 2048\n",
        "load_in_4bit = True\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9h4Qtsa5x1iJ",
        "outputId": "f36ca058-a95c-4ed6-bf49-52bbb15e5760"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Qwen2ForCausalLM(\n",
            "  (model): Qwen2Model(\n",
            "    (embed_tokens): Embedding(152064, 5120, padding_idx=151654)\n",
            "    (layers): ModuleList(\n",
            "      (0-4): 5 x Qwen2DecoderLayer(\n",
            "        (self_attn): Qwen2Attention(\n",
            "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
            "          (k_proj): Linear(in_features=5120, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=5120, out_features=1024, bias=True)\n",
            "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): Qwen2MLP(\n",
            "          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
            "          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
            "          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
            "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
            "      )\n",
            "      (5-6): 2 x Qwen2DecoderLayer(\n",
            "        (self_attn): Qwen2Attention(\n",
            "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
            "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
            "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
            "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): Qwen2MLP(\n",
            "          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
            "          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
            "          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
            "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
            "      )\n",
            "      (7-22): 16 x Qwen2DecoderLayer(\n",
            "        (self_attn): Qwen2Attention(\n",
            "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
            "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
            "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
            "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): Qwen2MLP(\n",
            "          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
            "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
            "      )\n",
            "      (23): Qwen2DecoderLayer(\n",
            "        (self_attn): Qwen2Attention(\n",
            "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
            "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
            "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
            "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): Qwen2MLP(\n",
            "          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
            "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
            "      )\n",
            "      (24-25): 2 x Qwen2DecoderLayer(\n",
            "        (self_attn): Qwen2Attention(\n",
            "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
            "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
            "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
            "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): Qwen2MLP(\n",
            "          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
            "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
            "      )\n",
            "      (26): Qwen2DecoderLayer(\n",
            "        (self_attn): Qwen2Attention(\n",
            "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
            "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
            "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
            "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): Qwen2MLP(\n",
            "          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
            "          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
            "          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
            "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
            "      )\n",
            "      (27-42): 16 x Qwen2DecoderLayer(\n",
            "        (self_attn): Qwen2Attention(\n",
            "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
            "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
            "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
            "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): Qwen2MLP(\n",
            "          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
            "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
            "      )\n",
            "      (43-46): 4 x Qwen2DecoderLayer(\n",
            "        (self_attn): Qwen2Attention(\n",
            "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
            "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
            "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
            "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): Qwen2MLP(\n",
            "          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
            "          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
            "          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
            "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
            "      )\n",
            "      (47): Qwen2DecoderLayer(\n",
            "        (self_attn): Qwen2Attention(\n",
            "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
            "          (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
            "          (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
            "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): Qwen2MLP(\n",
            "          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
            "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=5120, out_features=152064, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QM0Qekb9vGR4",
        "outputId": "d20c1d84-6f9c-4f4d-f88f-d91c78402928"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<think>\n",
            "The sequence starts with 1, 1, and then each subsequent number is the sum of the two preceding ones. After 5, the next number should be 8, which already exists in the provided sequence. Adding the next term involves adding the two previous numbers: 5 + 8 equals 13.\n",
            "\n",
            "Thus, the extended sequence includes the number 13 following 8.\n",
            "</think>\n",
            "\n",
            "Sure! Let's continue the Fibonacci sequence from where it left off:\n",
            "\n",
            "**Given sequence:**  \n",
            "1, 1, 2, 3, 5, 8,  \n",
            "**Next terms:**\n",
            "\n",
            "The Fibonacci\n"
          ]
        }
      ],
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(\n",
        "    input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
        "    use_cache = True, temperature = 1.5, min_p = 0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiWjRqFym0Ep",
        "outputId": "53c9200d-c42e-4853-9271-6b18ee8f1823"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<think>\n",
            "Okay, so I need to figure out how to respond to this. The user wrote a story starter: \"Once upon a time, there was a...\" and they provided a response from someone called \"the story whisperer\" who continued the story. The response included a story about a small coastal village, the old lighthouse keeper named Thomas, his dog Max, a mysterious fog, and a little girl named Clara who could communicate with the sea. The user then wrote their own version, adding details like the wooden lighthouse with a rusty iron staircase, the creaking floorboards, the salty tang, and the foghorn\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Once upon a time, there was a\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(\n",
        "    input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
        "    use_cache = True, temperature = 1.5, min_p = 0.1\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
