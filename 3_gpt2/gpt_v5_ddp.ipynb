{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.chdir('..')\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "# from transformers import GPT2LMHeadModel\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "\n",
    "# from tqdm import tqdm, trange\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from utils import *; from boring_utils.utils import *\n",
    "from data_structure import add_to_class\n",
    "\n",
    "from hf_gpt import (\n",
    "    GPT, \n",
    "    GPTConfig,\n",
    "    GPTConfig_small\n",
    ")\n",
    "\n",
    "from dataloader import (\n",
    "    DataLoaderTiny\n",
    ")\n",
    "\n",
    "init_graph()\n",
    "device = get_device()\n",
    "\n",
    "def reset_model_weights(model):\n",
    "    for layer in model.modules():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "train_loader = DataLoaderTiny(B=4, T=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 10\n",
    "max_steps = 50\n",
    "\n",
    "total_batch_size = 524288  # 2 ** 19, ~0.5M in number of tokens\n",
    "B = 16  # micro batch size\n",
    "T = 1024  # seq len\n",
    "assert total_batch_size % (B * T) == 0\n",
    "grad_accum_steps = total_batch_size // (B * T)  # in this case, 32\n",
    "cprint(grad_accum_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_model_weights(model)\n",
    "optimizer = model.configure_optimizers(\n",
    "    weight_decay=0.1, learning_rate=6e-4, device_type=\"cuda\")\n",
    "\n",
    "pbar = tqdm(range(max_steps), desc=\"Training\")\n",
    "for i in pbar:\n",
    "    t0 = time.time()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # grad accumulation\n",
    "    loss_accum = 0.0\n",
    "    # grad_accum_steps = total_batch_size // (B * T)\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        x, y = train_loader.next_batch()\n",
    "\n",
    "        # allow regions of script to run in mixed precision\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            logits, loss = model(x.to(device), y.to(device))\n",
    "\n",
    "        # we have to scale the loss to account for gradient accumulation\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward()\n",
    "\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    lr = get_lr(i, max_lr, warmup_steps, max_steps, min_lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0  # time difference in seconds\n",
    "    tokens_processed = train_loader.B * train_loader.T\n",
    "    tokens_per_sec = tokens_processed / dt\n",
    "\n",
    "    pbar.set_description(f\"Step {i}, Loss: {loss.item():.4f}, LR: {lr:.1e}, Tokens/s: {tokens_per_sec:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
