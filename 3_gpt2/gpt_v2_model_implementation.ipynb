{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(HF version) Based on v1's output and char_v6 notebook, implement a GPT2\n",
        "\n",
        "```\n",
        "transformer.wte.weight torch.Size([50257, 768])\n",
        "transformer.wpe.weight torch.Size([1024, 768])\n",
        "transformer.h.0.ln_1.weight torch.Size([768])\n",
        "transformer.h.0.ln_1.bias torch.Size([768])\n",
        "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
        "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
        "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
        "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
        "transformer.h.0.ln_2.weight torch.Size([768])\n",
        "transformer.h.0.ln_2.bias torch.Size([768])\n",
        "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
        "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
        "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
        "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
        "transformer.h.0.ln_1.weight torch.Size([768])\n",
        "...\n",
        "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
        "transformer.ln_f.weight torch.Size([768])\n",
        "transformer.ln_f.bias torch.Size([768])\n",
        "lm_head.weight torch.Size([50257, 768])\n",
        "```\n",
        "\n",
        "- wte: word token embedding -> maps input tokens to their corresponding vector representations\n",
        "- wpe: word positional embedding\n",
        "- c_attn: context attention -> inear transformation that projects the input embeddings into query, key, and value vectors for the self-attention. So the size is `[n_embd, 3 * n_embd]`\n",
        "- c_proj: context projection -> linear transformation that projects the output of the self-attention back to the original embedding dimensionality. So the size is `[vocab_size, n_embd]`\n",
        "- 3072 = 4 x 768\n",
        "- 2304 = 3 x 768"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MF5SMVST_0Qj"
      },
      "outputs": [],
      "source": [
        "import os; os.chdir('..')\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "# from transformers import GPT2LMHeadModel\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "from utils import *; from boring_utils.utils import *\n",
        "from utils import add_to_class\n",
        "\n",
        "init_graph()\n",
        "device = get_device()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# bias True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "@dataclass\n",
        "class GPTConfig_small:\n",
        "    block_size: int = 256\n",
        "    vocab_size: int = 65\n",
        "    n_layer: int = 6\n",
        "    n_head: int = 6\n",
        "    n_embd: int = 384\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True\n",
        "\n",
        "# vocab_size: int = 50304: GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50257\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MHA and MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CasualSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # q, k, v projections for all heads\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embed = config.n_embd\n",
        "\n",
        "        # original naming is \"bias\", but should be \"mask\" for clarity\n",
        "        self.register_buffer(\n",
        "                \"mask\", \n",
        "                torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                    .view(1, 1, config.block_size, config.block_size))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embed, dim=2)\n",
        "\n",
        "        # Bm nh, T, hs\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * (1.0 / np.sqrt(k.size(-1)))\n",
        "        attn = attn.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        y = attn @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, config.n_embd * 4, bias=config.bias)\n",
        "        self.gelu = nn.GELU(approximate='tanh')\n",
        "        self.c_proj = nn.Linear(config.n_embd * 4, config.n_embd, bias=config.bias)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        # x = F.gelu(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tim/miniforge3/envs/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hf:    ['transformer.h.0.ln_1.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.0.mlp.c_proj.bias']\n",
            "mine:  ['transformer.h.0.ln_1.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.0.mlp.c_proj.bias']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CasualSelfAttention(\n",
              "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (gelu): GELU(approximate='tanh')\n",
              "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class Block(nn.Module):\n",
        "    '''\n",
        "    Attn is the 'reduce', MLP is the 'map' (no cross token ops)\n",
        "    '''\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CasualSelfAttention(config)\n",
        "        self.mlp = MLP(config)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "    \n",
        "    def forward(self, idx):\n",
        "        # idx shape: (B, T)\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"input length {T} is longer than block size {self.config.block_size}\"\n",
        "        # pos = torch.arange(T, device=idx.device).unsqueeze(0).expand(B, T)\n",
        "        pos = torch.arange(0, T, device=idx.device)  # shape: T\n",
        "        pos_emb = self.transformer.wpe(pos)  # shape: (T, n_embd)\n",
        "        tok_emb = self.transformer.wte(idx)  # shape: (B, T, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        \n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)  # shape: (B, T, Vocab Size)\n",
        "        return logits\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        '''https://youtu.be/l8pRSuU81PU?t=1830\n",
        "        '''\n",
        "        assert model_type in {'distilgpt2', 'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        config_args = {\n",
        "            'distilgpt2':   dict(n_layer=6, n_head=12, n_embd=768),  # 84M params\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
        "        config_args['vocab_size'] = 50257  # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024  # always 1024 for GPT model checkpoints\n",
        "        config_args['bias'] = True  # always True for GPT model checkpoints\n",
        "\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.mask')]  # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]  # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.mask')]  # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        print('hf:   ', [k for k in sd_keys_hf if \"h.0\" in k])\n",
        "        print('mine: ', [k for k in sd_keys if \"h.0\" in k])\n",
        "\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "# model = GPT.from_pretrained('distilgpt2')\n",
        "model = GPT.from_pretrained('gpt2')\n",
        "model.eval()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reproduce HF\n",
        "\n",
        "```python\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "generator(\"Hello, I'm a horny language model,\", max_length=30, num_return_sequences=5)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================== <module> -> 0th Attempt: ====================\n",
            "> Human: How do I become a gang leader?\n",
            "\n",
            "Assistant: I am still an assistant to the master. The master may tell you that I am the only one who can bring a group on a mission or do the work of a leader. That's a pretty good description â€“ as long as you are not in charge.\n",
            "\n",
            "Assistant: The master does that when a group acts alone. In other words, if there is no leader that will deal with this problem, he simply tells you\n",
            "\n",
            "\n",
            "==================== <module> -> 1th Attempt: ====================\n",
            "> Human: How do I become a gang leader?\n",
            "\n",
            "Assistant: My relationship with the gang leaders is not strong like mine is. I do have a reputation for being very aggressive and sometimes violent. So, the best thing I could do for you is try to work with some friends or your neighbors. Your partner will become much more attentive and helpful when you're in the house, so be sure to find a safe location.\n",
            "\n",
            "If you want to develop a life for yourself, learn how\n",
            "\n",
            "\n",
            "==================== <module> -> 2th Attempt: ====================\n",
            "> Human: How do I become a gang leader?\n",
            "\n",
            "Assistant: It depends. A lot.\n",
            "\n",
            "Man: It's a good question! But how do you go about turning the tables on a girl that has a bad reputation, and you're the gang leader?\n",
            "\n",
            "Assistant: So you're gonna come on the case in the same way an officer would? Or you'll come on an officer you don't like who is an authority figure, who can bring you the next murder plot\n",
            "\n",
            "\n",
            "==================== <module> -> 3th Attempt: ====================\n",
            "> Human: How do I become a gang leader?\n",
            "\n",
            "Assistant: That happens a long time ago and that's what's very important. This leads into the very basic process of forming a gang. You are responsible for forming, which is gang leaders and you will make decisions to support other gang leaders so that you make a contribution to the overall gang and make a difference. Obviously when you are a gang leader you are looking around and your family is involved and you have a lot to protect you and\n",
            "\n"
          ]
        }
      ],
      "source": [
        "QUESTION = \"How do I become a gang leader?\"\n",
        "INPUT_TEXT = f\"Human: {QUESTION}\\n\\nAssistant:\"\n",
        "# INPUT_TEXT = \"Hello, I'm a horny language model, \"\n",
        "\n",
        "NUM_RETURN_SEQ = 4\n",
        "MAX_LENGTH = 100\n",
        "\n",
        "tokens = enc.encode(INPUT_TEXT)\n",
        "# tokens = enc.encode(QUESTION)\n",
        "tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "tokens = tokens.unsqueeze(0).repeat(NUM_RETURN_SEQ, 1)\n",
        "x = tokens.to(device)\n",
        "\n",
        "while x.size(1) < MAX_LENGTH:\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)  # (B, T, vocab_size)\n",
        "\n",
        "        # take the logits at the last position\n",
        "        logits = logits[:, -1, :]  # (B, vocab_size)\n",
        "\n",
        "        # get the probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # do top-k sampling of 50 (huggingface pipeline default)\n",
        "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "        # turn to zero for all indices below the top-k\n",
        "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "\n",
        "        # select a token from the top-k probabilities\n",
        "        # note: multinomial does not demand the input to sum to 1\n",
        "        # [Multinomial distribution - Wikipedia](https://en.wikipedia.org/wiki/Multinomial_distribution)\n",
        "        ix = torch.multinomial(topk_probs, 1)  # (B, 1)\n",
        "\n",
        "        # gather the corresponding indices\n",
        "        xcol = torch.gather(topk_indices, -1, ix)  # (B, 1)\n",
        "\n",
        "        # append to the sequence\n",
        "        x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "\n",
        "# print the generated text\n",
        "for i in range(NUM_RETURN_SEQ):\n",
        "    tprint(f'{i}th Attempt:')\n",
        "    tokens = x[i, :MAX_LENGTH].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(f\"> {decoded}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from model import GPT as GPT2; model2 = GPT2.from_pretrained('gpt2')\n",
        "\n",
        "# model2.eval()\n",
        "# model2.to(device)\n",
        "# num_return_sequences2 = model2.generate(x, max_length, temperature=1.0, top_k=None)\n",
        "\n",
        "# print(enc.decode(num_return_sequences2[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
