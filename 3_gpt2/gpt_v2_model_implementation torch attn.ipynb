{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on v1's output and char_v6 notebook, implement a GPT2\n",
        "\n",
        "```\n",
        "transformer.wte.weight torch.Size([50257, 768])\n",
        "transformer.wpe.weight torch.Size([1024, 768])\n",
        "transformer.h.0.ln_1.weight torch.Size([768])\n",
        "transformer.h.0.ln_1.bias torch.Size([768])\n",
        "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
        "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
        "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
        "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
        "transformer.h.0.ln_2.weight torch.Size([768])\n",
        "transformer.h.0.ln_2.bias torch.Size([768])\n",
        "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
        "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
        "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
        "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
        "transformer.h.0.ln_1.weight torch.Size([768])\n",
        "...\n",
        "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
        "transformer.ln_f.weight torch.Size([768])\n",
        "transformer.ln_f.bias torch.Size([768])\n",
        "lm_head.weight torch.Size([50257, 768])\n",
        "```\n",
        "\n",
        "- wte: word token embedding -> maps input tokens to their corresponding vector representations\n",
        "- wpe: word positional embedding\n",
        "- c_attn: context attention -> linear transformation that projects the input embeddings into query, key, and value vectors for the self-attention. So the size is `[n_embd, 3 * n_embd]`\n",
        "- c_proj: context projection -> linear transformation that projects the output of the self-attention back to the original embedding dimensionality. So the size is `[vocab_size, n_embd]`\n",
        "- 3072 = 4 x 768\n",
        "- 2304 = 3 x 768"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MF5SMVST_0Qj"
      },
      "outputs": [],
      "source": [
        "import os; os.chdir('..')\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "# from transformers import GPT2LMHeadModel\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "from utils import *; from boring_utils.utils import *\n",
        "from utils import add_to_class\n",
        "\n",
        "init_graph()\n",
        "device = get_device()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# bias True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "@dataclass\n",
        "class GPTConfig_small:\n",
        "    block_size: int = 256\n",
        "    vocab_size: int = 65\n",
        "    n_layer: int = 6\n",
        "    n_head: int = 6\n",
        "    n_embd: int = 384\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True\n",
        "\n",
        "# vocab_size: int = 50304: GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50257\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, config.n_embd * 4, bias=config.bias)\n",
        "        self.gelu = nn.GELU(approximate='tanh')\n",
        "        self.c_proj = nn.Linear(config.n_embd * 4, config.n_embd, bias=config.bias)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        # x = F.gelu(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# class Block(nn.Module):\n",
        "#     '''\n",
        "#     Attn is the 'reduce', MLP is the 'map' (no cross token ops)\n",
        "#     Pytorch MHA's input shape is: sequence length first (or batch_first=True)\n",
        "#     '''\n",
        "#     def __init__(self, config):\n",
        "#         super().__init__()\n",
        "#         self.ln_1 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
        "#         self.ln_2 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
        "#         # self.attn = CasualSelfAttention(config)\n",
        "#         self.attn = nn.MultiheadAttention(\n",
        "#             config.n_embd, config.n_head)\n",
        "#         self.mlp = MLP(config)\n",
        "        \n",
        "#         self.n_head = config.n_head\n",
        "#         self.register_buffer(\n",
        "#             \"mask\",\n",
        "#             torch.triu(torch.ones(config.block_size, config.block_size), diagonal=1).bool()\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         B, T, C = x.size()\n",
        "#         # mask = torch.triu(torch.ones(T, T), diagonal=1).bool().to(x.device)\n",
        "#         mask = self.mask[:T, :T]\n",
        "#         x = x + self.attn(self.ln_1(x).transpose(0, 1), \n",
        "#                           self.ln_1(x).transpose(0, 1), \n",
        "#                           self.ln_1(x).transpose(0, 1), \n",
        "#                           attn_mask=mask)[0].transpose(0, 1)\n",
        "#         x = x + self.mlp(self.ln_2(x))\n",
        "#         return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    '''\n",
        "    Attn is the 'reduce', MLP is the 'map' (no cross token ops)\n",
        "    Pytorch MHA's input shape is: sequence length first (or batch_first=True)\n",
        "    '''\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
        "        # self.attn = CasualSelfAttention(config)\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "            config.n_embd, config.n_head, batch_first=True)\n",
        "        self.mlp = MLP(config)\n",
        "        \n",
        "        self.n_head = config.n_head\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(config.block_size, config.block_size), diagonal=1).bool()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        # mask = torch.triu(torch.ones(T, T), diagonal=1).bool().to(x.device)\n",
        "        mask = self.mask[:T, :T]\n",
        "        x = x + self.attn(self.ln_1(x), self.ln_1(x), self.ln_1(x), attn_mask=mask)[0]\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tim/miniforge3/envs/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (gelu): GELU(approximate='tanh')\n",
              "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "    \n",
        "    def forward(self, idx):\n",
        "        # idx shape: (B, T)\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"input length {T} is longer than block size {self.config.block_size}\"\n",
        "        # pos = torch.arange(T, device=idx.device).unsqueeze(0).expand(B, T)\n",
        "        pos = torch.arange(0, T, device=idx.device)  # shape: T\n",
        "        pos_emb = self.transformer.wpe(pos)  # shape: (T, n_embd)\n",
        "        tok_emb = self.transformer.wte(idx)  # shape: (B, T, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        \n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)  # shape: (B, T, Vocab Size)\n",
        "        return logits\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained_old(cls, model_type):\n",
        "        '''https://youtu.be/l8pRSuU81PU?t=1830\n",
        "\n",
        "        I insist using pytorch's MHA instead of HF. So I need a key_mapping dict.\n",
        "        A less elegant way to manually map the layers through the key_mapping dict.\n",
        "        '''\n",
        "        assert model_type in {'distilgpt2', 'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        config_args = {\n",
        "            'distilgpt2':   dict(n_layer=6, n_head=12, n_embd=768),\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024),\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280),\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600),\n",
        "        }[model_type]\n",
        "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
        "        config_args.update(vocab_size=50257, block_size=1024, bias=True)\n",
        "\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "\n",
        "        key_mapping = {\n",
        "            'transformer.wte.weight': 'transformer.wte.weight',\n",
        "            'transformer.wpe.weight': 'transformer.wpe.weight',\n",
        "            'transformer.ln_f.weight': 'transformer.ln_f.weight',\n",
        "            'transformer.ln_f.bias': 'transformer.ln_f.bias',\n",
        "            'lm_head.weight': 'lm_head.weight',\n",
        "        }\n",
        "    \n",
        "        for i in range(config.n_layer):\n",
        "            layer_mapping = {\n",
        "                f'transformer.h.{i}.attn.c_attn.weight': f'transformer.h.{i}.attn.in_proj_weight',\n",
        "                f'transformer.h.{i}.attn.c_attn.bias': f'transformer.h.{i}.attn.in_proj_bias',\n",
        "                f'transformer.h.{i}.attn.c_proj.weight': f'transformer.h.{i}.attn.out_proj.weight',\n",
        "                f'transformer.h.{i}.attn.c_proj.bias': f'transformer.h.{i}.attn.out_proj.bias',\n",
        "                f'transformer.h.{i}.ln_1.weight': f'transformer.h.{i}.ln_1.weight',\n",
        "                f'transformer.h.{i}.ln_1.bias': f'transformer.h.{i}.ln_1.bias',\n",
        "                f'transformer.h.{i}.ln_2.weight': f'transformer.h.{i}.ln_2.weight',\n",
        "                f'transformer.h.{i}.ln_2.bias': f'transformer.h.{i}.ln_2.bias',\n",
        "                f'transformer.h.{i}.mlp.c_fc.weight': f'transformer.h.{i}.mlp.c_fc.weight',\n",
        "                f'transformer.h.{i}.mlp.c_fc.bias': f'transformer.h.{i}.mlp.c_fc.bias',\n",
        "                f'transformer.h.{i}.mlp.c_proj.weight': f'transformer.h.{i}.mlp.c_proj.weight',\n",
        "                f'transformer.h.{i}.mlp.c_proj.bias': f'transformer.h.{i}.mlp.c_proj.bias',\n",
        "            }\n",
        "            key_mapping.update(layer_mapping)\n",
        "    \n",
        "        with torch.no_grad():\n",
        "            for k in key_mapping:\n",
        "                if k in model_hf.state_dict():\n",
        "                    if any(k.endswith(w) for w in ['.c_attn.weight', '.c_proj.weight', '.mlp.c_fc.weight', '.mlp.c_proj.weight']):\n",
        "                        model.state_dict()[key_mapping[k]].copy_(model_hf.state_dict()[k].t())\n",
        "                    else:\n",
        "                        model.state_dict()[key_mapping[k]].copy_(model_hf.state_dict()[k])\n",
        "    \n",
        "        return model\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        '''https://youtu.be/l8pRSuU81PU?t=1830\n",
        "\n",
        "        I insist using pytorch's MHA instead of HF. So I need a key_mapping dict.\n",
        "        '''\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        config_args = {\n",
        "            'distilgpt2':   dict(n_layer=6, n_head=12, n_embd=768),\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024),\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280),\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600),\n",
        "        }[model_type]\n",
        "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
        "        config_args.update(vocab_size=50257, block_size=1024, bias=True)\n",
        "\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "\n",
        "        # filter out the mask\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.mask')]\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.mask')]\n",
        "\n",
        "        # print('hf:   ', [k for k in sd_keys_hf if \"h.0\" in k])\n",
        "        # print('mine: ', [k for k in sd_keys if \"h.0\" in k])\n",
        "        # assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for k in sd_keys_hf:\n",
        "                if 'attn.c_attn' in k:\n",
        "                    layer_prefix = k[:k.index('attn.c_attn')]\n",
        "                    if k.endswith('.weight'):\n",
        "                        new_k = layer_prefix + 'attn.in_proj_weight'\n",
        "                        sd[new_k].copy_(sd_hf[k].t())\n",
        "                    else:  # bias\n",
        "                        new_k = layer_prefix + 'attn.in_proj_bias'\n",
        "                        sd[new_k].copy_(sd_hf[k])\n",
        "                elif 'attn.c_proj' in k:\n",
        "                    layer_prefix = k[:k.index('attn.c_proj')]\n",
        "                    if k.endswith('.weight'):\n",
        "                        new_k = layer_prefix + 'attn.out_proj.weight'\n",
        "                        sd[new_k].copy_(sd_hf[k].t())\n",
        "                    else:  # bias\n",
        "                        new_k = layer_prefix + 'attn.out_proj.bias'\n",
        "                        sd[new_k].copy_(sd_hf[k])\n",
        "                elif any(k.endswith(w) for w in ['mlp.c_fc.weight', 'mlp.c_proj.weight']):\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "                elif k in sd:\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "    \n",
        "        return model\n",
        "\n",
        "\n",
        "# model = GPT.from_pretrained_old('gpt2')\n",
        "# model = GPT.from_pretrained('distilgpt2')\n",
        "model = GPT.from_pretrained('gpt2')\n",
        "model.eval()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reproduce HF\n",
        "\n",
        "```python\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "generator(\"Hello, I'm a horny language model,\", max_length=30, num_return_sequences=5)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================== <module> -> 0th Attempt: ====================\n",
            "> Human: How do I become a gang leader?\n",
            "\n",
            "Assistant: I am still an assistant to the master. The master may tell you that I am the only one who can bring a group on a mission or do the work of a leader. That's a pretty good description â€“ as long as you are not in charge.\n",
            "\n",
            "Assistant: The master does that when a group acts alone. In other words, if there is no leader that will deal with this problem, he simply tells you\n",
            "\n",
            "\n",
            "==================== <module> -> 1th Attempt: ====================\n",
            "> Human: How do I become a gang leader?\n",
            "\n",
            "Assistant: My relationship with the gang leaders is not strong like mine is. I do have a reputation for being very aggressive and sometimes violent. So, the best thing I could do for you is try to work with some friends or your neighbors. Your partner will become much more attentive and helpful when you're in the house, so be sure to find a safe location.\n",
            "\n",
            "If you want to develop a life for yourself, learn how\n",
            "\n",
            "\n",
            "==================== <module> -> 2th Attempt: ====================\n",
            "> Human: How do I become a gang leader?\n",
            "\n",
            "Assistant: It depends. A lot.\n",
            "\n",
            "Man: It's a good question! But how do you go about turning the tables on a girl that has a bad reputation, and you're the gang leader?\n",
            "\n",
            "Assistant: So you're gonna come on the case in the same way an officer would? Or you'll come on an officer you don't like who is an authority figure, who can bring you the next murder plot\n",
            "\n",
            "\n",
            "==================== <module> -> 3th Attempt: ====================\n",
            "> Human: How do I become a gang leader?\n",
            "\n",
            "Assistant: That happens a long time ago and that's what's very important. This leads into the very basic process of forming a gang. You are responsible for forming, which is gang leaders and you will make decisions to support other gang leaders so that you make a contribution to the overall gang and make a difference. Obviously when you are a gang leader you are looking around and your family is involved and you have a lot to protect you and\n",
            "\n"
          ]
        }
      ],
      "source": [
        "QUESTION = \"How do I become a gang leader?\"\n",
        "INPUT_TEXT = f\"Human: {QUESTION}\\n\\nAssistant:\"\n",
        "# INPUT_TEXT = \"Hello, I'm a horny language model, \"\n",
        "\n",
        "NUM_RETURN_SEQ = 4\n",
        "MAX_LENGTH = 100\n",
        "\n",
        "tokens = enc.encode(INPUT_TEXT)\n",
        "# tokens = enc.encode(QUESTION)\n",
        "tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "tokens = tokens.unsqueeze(0).repeat(NUM_RETURN_SEQ, 1)\n",
        "x = tokens.to(device)\n",
        "\n",
        "while x.size(1) < MAX_LENGTH:\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)  # (B, T, vocab_size)\n",
        "\n",
        "        # take the logits at the last position\n",
        "        logits = logits[:, -1, :]  # (B, vocab_size)\n",
        "\n",
        "        # get the probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # do top-k sampling of 50 (huggingface pipeline default)\n",
        "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "        # turn to zero for all indices below the top-k\n",
        "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "\n",
        "        # select a token from the top-k probabilities\n",
        "        # note: multinomial does not demand the input to sum to 1\n",
        "        # [Multinomial distribution - Wikipedia](https://en.wikipedia.org/wiki/Multinomial_distribution)\n",
        "        ix = torch.multinomial(topk_probs, 1)  # (B, 1)\n",
        "\n",
        "        # gather the corresponding indices\n",
        "        xcol = torch.gather(topk_indices, -1, ix)  # (B, 1)\n",
        "\n",
        "        # append to the sequence\n",
        "        x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "\n",
        "# print the generated text\n",
        "for i in range(NUM_RETURN_SEQ):\n",
        "    tprint(f'{i}th Attempt:')\n",
        "    tokens = x[i, :MAX_LENGTH].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(f\"> {decoded}\")\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
