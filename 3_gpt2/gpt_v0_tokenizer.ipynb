{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.chdir('..')\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *; from boring_utils.utils import *\n",
    "\n",
    "init_graph()\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ref\n",
    "huggingface/transformers PyTorch implementation:\n",
    "- https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "- https://huggingface.co/distilbert/distilgpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_args = {\n",
    "    'distilgpt2': dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "}['distilgpt2']\n",
    "\n",
    "config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "config_args['bias'] = True # always True for GPT model checkpoints\n",
    "\n",
    "model_hf = GPT2LMHeadModel.from_pretrained('distilgpt2').to(device)\n",
    "sd_hf = model_hf.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93mPublic Methods:\u001b[0m\n",
      "    T_destination\n",
      "    active_adapter\n",
      "    active_adapters\n",
      "    add_adapter\n",
      "    add_memory_hooks\n",
      "    add_model_tags\n",
      "    add_module\n",
      "    apply\n",
      "    assisted_decoding\n",
      "    base_model\n",
      "    base_model_prefix\n",
      "    beam_sample\n",
      "    beam_search\n",
      "    bfloat16\n",
      "    buffers\n",
      "    call_super_init\n",
      "    can_generate\n",
      "    children\n",
      "    compile\n",
      "    compute_transition_scores\n",
      "    config\n",
      "    config_class\n",
      "    constrained_beam_search\n",
      "    contrastive_search\n",
      "    cpu\n",
      "    create_extended_attention_mask_for_decoder\n",
      "    cuda\n",
      "    deparallelize\n",
      "    device\n",
      "    device_map\n",
      "    disable_adapters\n",
      "    disable_input_require_grads\n",
      "    double\n",
      "    dtype\n",
      "    dummy_inputs\n",
      "    dump_patches\n",
      "    enable_adapters\n",
      "    enable_input_require_grads\n",
      "    estimate_tokens\n",
      "    eval\n",
      "    extra_repr\n",
      "    float\n",
      "    floating_point_ops\n",
      "    forward\n",
      "    framework\n",
      "    from_pretrained\n",
      "    generate\n",
      "    generation_config\n",
      "    get_adapter_state_dict\n",
      "    get_buffer\n",
      "    get_extended_attention_mask\n",
      "    get_extra_state\n",
      "    get_head_mask\n",
      "    get_input_embeddings\n",
      "    get_memory_footprint\n",
      "    get_output_embeddings\n",
      "    get_parameter\n",
      "    get_position_embeddings\n",
      "    get_submodule\n",
      "    gradient_checkpointing_disable\n",
      "    gradient_checkpointing_enable\n",
      "    greedy_search\n",
      "    group_beam_search\n",
      "    half\n",
      "    init_weights\n",
      "    invert_attention_mask\n",
      "    ipu\n",
      "    is_gradient_checkpointing\n",
      "    is_loaded_in_4bit\n",
      "    is_loaded_in_8bit\n",
      "    is_parallelizable\n",
      "    lm_head\n",
      "    load_adapter\n",
      "    load_state_dict\n",
      "    load_tf_weights\n",
      "    main_input_name\n",
      "    model_parallel\n",
      "    model_tags\n",
      "    modules\n",
      "    name_or_path\n",
      "    named_buffers\n",
      "    named_children\n",
      "    named_modules\n",
      "    named_parameters\n",
      "    num_parameters\n",
      "    parallelize\n",
      "    parameters\n",
      "    post_init\n",
      "    prepare_inputs_for_generation\n",
      "    prune_heads\n",
      "    push_to_hub\n",
      "    register_backward_hook\n",
      "    register_buffer\n",
      "    register_for_auto_class\n",
      "    register_forward_hook\n",
      "    register_forward_pre_hook\n",
      "    register_full_backward_hook\n",
      "    register_full_backward_pre_hook\n",
      "    register_load_state_dict_post_hook\n",
      "    register_module\n",
      "    register_parameter\n",
      "    register_state_dict_pre_hook\n",
      "    requires_grad_\n",
      "    reset_memory_hooks_state\n",
      "    resize_position_embeddings\n",
      "    resize_token_embeddings\n",
      "    retrieve_modules_from_names\n",
      "    reverse_bettertransformer\n",
      "    sample\n",
      "    save_pretrained\n",
      "    set_adapter\n",
      "    set_extra_state\n",
      "    set_input_embeddings\n",
      "    set_output_embeddings\n",
      "    share_memory\n",
      "    state_dict\n",
      "    supports_gradient_checkpointing\n",
      "    tie_weights\n",
      "    to\n",
      "    to_bettertransformer\n",
      "    to_empty\n",
      "    train\n",
      "    training\n",
      "    transformer\n",
      "    type\n",
      "    warn_if_padding_and_no_attention_mask\n",
      "    warnings_issued\n",
      "    xpu\n",
      "    zero_grad\n"
     ]
    }
   ],
   "source": [
    "mprint(model_hf, magic_methods=False, private_methods=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load the tokenizer for the distilgpt2 model\n",
    "tokenizer_hf = GPT2Tokenizer.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93mPublic Methods:\u001b[0m\n",
      "    SPECIAL_TOKENS_ATTRIBUTES\n",
      "    add_bos_token\n",
      "    add_prefix_space\n",
      "    add_special_tokens\n",
      "    add_tokens\n",
      "    added_tokens_decoder\n",
      "    added_tokens_encoder\n",
      "    additional_special_tokens\n",
      "    additional_special_tokens_ids\n",
      "    all_special_ids\n",
      "    all_special_tokens\n",
      "    all_special_tokens_extended\n",
      "    apply_chat_template\n",
      "    as_target_tokenizer\n",
      "    batch_decode\n",
      "    batch_encode_plus\n",
      "    bos_token\n",
      "    bos_token_id\n",
      "    bpe\n",
      "    bpe_ranks\n",
      "    build_inputs_with_special_tokens\n",
      "    byte_decoder\n",
      "    byte_encoder\n",
      "    cache\n",
      "    chat_template\n",
      "    clean_up_tokenization\n",
      "    clean_up_tokenization_spaces\n",
      "    cls_token\n",
      "    cls_token_id\n",
      "    convert_added_tokens\n",
      "    convert_ids_to_tokens\n",
      "    convert_tokens_to_ids\n",
      "    convert_tokens_to_string\n",
      "    create_token_type_ids_from_sequences\n",
      "    decode\n",
      "    decoder\n",
      "    default_chat_template\n",
      "    deprecation_warnings\n",
      "    encode\n",
      "    encode_plus\n",
      "    encoder\n",
      "    eos_token\n",
      "    eos_token_id\n",
      "    errors\n",
      "    from_pretrained\n",
      "    get_added_vocab\n",
      "    get_special_tokens_mask\n",
      "    get_vocab\n",
      "    init_inputs\n",
      "    init_kwargs\n",
      "    is_fast\n",
      "    mask_token\n",
      "    mask_token_id\n",
      "    max_len_sentences_pair\n",
      "    max_len_single_sentence\n",
      "    max_model_input_sizes\n",
      "    model_input_names\n",
      "    model_max_length\n",
      "    name_or_path\n",
      "    num_special_tokens_to_add\n",
      "    pad\n",
      "    pad_token\n",
      "    pad_token_id\n",
      "    pad_token_type_id\n",
      "    padding_side\n",
      "    pat\n",
      "    prepare_for_model\n",
      "    prepare_for_tokenization\n",
      "    prepare_seq2seq_batch\n",
      "    pretrained_init_configuration\n",
      "    pretrained_vocab_files_map\n",
      "    push_to_hub\n",
      "    register_for_auto_class\n",
      "    sanitize_special_tokens\n",
      "    save_pretrained\n",
      "    save_vocabulary\n",
      "    sep_token\n",
      "    sep_token_id\n",
      "    slow_tokenizer_class\n",
      "    special_tokens_map\n",
      "    special_tokens_map_extended\n",
      "    split_special_tokens\n",
      "    tokenize\n",
      "    tokens_trie\n",
      "    truncate_sequences\n",
      "    truncation_side\n",
      "    unk_token\n",
      "    unk_token_id\n",
      "    verbose\n",
      "    vocab_files_names\n",
      "    vocab_size\n"
     ]
    }
   ],
   "source": [
    "mprint(tokenizer_hf, magic_methods=False, private_methods=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mtokenizer_hf\u001b[0m: \n",
      "GPT2Tokenizer(name_or_path='distilgpt2', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the tokenizer\n",
    "cprint(tokenizer_hf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview the Vocab Dict\n",
    "\n",
    "- `Ġ` means the whitespace before the word\n",
    "- No explicit <PAD>, <CLS>, <SEP> tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mlen(sorted_vocab)\u001b[0m: \n",
      "50257\n",
      "\n",
      "! 0\n",
      "\" 1\n",
      "# 2\n",
      "$ 3\n",
      "% 4\n",
      "& 5\n",
      "' 6\n",
      "( 7\n",
      ") 8\n",
      "* 9\n",
      "+ 10\n",
      ", 11\n",
      "- 12\n",
      ". 13\n",
      "/ 14\n",
      "0 15\n",
      "1 16\n",
      "2 17\n",
      "3 18\n",
      "4 19\n",
      "====================\n",
      "Ġprodu 990\n",
      "Ġstill 991\n",
      "led 992\n",
      "ah 993\n",
      "Ġhere 994\n",
      "Ġworld 995\n",
      "Ġthough 996\n",
      "Ġnum 997\n",
      "arch 998\n",
      "imes 999\n",
      "ale 1000\n",
      "ĠSe 1001\n",
      "ĠIf 1002\n",
      "// 1003\n",
      "ĠLe 1004\n",
      "Ġret 1005\n",
      "Ġref 1006\n",
      "Ġtrans 1007\n",
      "ner 1008\n",
      "ution 1009\n",
      "====================\n",
      "Revolution 50237\n",
      "Ġsnipers 50238\n",
      "Ġreverted 50239\n",
      "Ġconglomerate 50240\n",
      "Terry 50241\n",
      "794 50242\n",
      "Ġharsher 50243\n",
      "Ġdesolate 50244\n",
      "ĠHitman 50245\n",
      "Commission 50246\n",
      "Ġ(/ 50247\n",
      "âĢ¦.\" 50248\n",
      "Compar 50249\n",
      "Ġamplification 50250\n",
      "ominated 50251\n",
      "Ġregress 50252\n",
      "ĠCollider 50253\n",
      "Ġinformants 50254\n",
      "Ġgazed 50255\n",
      "<|endoftext|> 50256\n"
     ]
    }
   ],
   "source": [
    "# Access and print the vocabulary items\n",
    "vocab_dict = tokenizer_hf.get_vocab()\n",
    "vocab_items = vocab_dict.items()\n",
    "\n",
    "sorted_vocab = sorted(vocab_items, key=lambda item: item[1])  # Sorting by token ID for readability\n",
    "\n",
    "cprint(len(sorted_vocab))\n",
    "\n",
    "for token, id in sorted_vocab[:20]:\n",
    "    print(token, id)\n",
    "\n",
    "print('=' * 20) \n",
    "\n",
    "for token, id in sorted_vocab[990:1010]:\n",
    "    print(token, id)\n",
    "\n",
    "print('=' * 20) \n",
    "\n",
    "for token, id in sorted_vocab[-20:]:\n",
    "    print(token, id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decode Some Random Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mtokenizer_hf.convert_ids_to_tokens(token_ids)\u001b[0m: \n",
      "['Hello', ',', 'Ġworld', '!']\n",
      "\n",
      "====================\n",
      "\u001b[93mtokenizer_hf.convert_ids_to_tokens(token_ids)\u001b[0m: \n",
      "['ĠHello']\n",
      "\n",
      "====================\n",
      "\u001b[93mtokenizer_hf.convert_ids_to_tokens(token_ids)\u001b[0m: \n",
      "['hello']\n",
      "\n",
      "====================\n",
      "\u001b[93mtokenizer_hf.convert_ids_to_tokens(token_ids)\u001b[0m: \n",
      "['Ġhello']\n",
      "\n",
      "====================\n",
      "\u001b[93mtokenizer_hf.convert_ids_to_tokens(token_ids)\u001b[0m: \n",
      "['568', '73', '+', '318', '46', '23', '=', '123', '45', '67', '89', '-', '1', '000000', '000']\n",
      "\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "text_li = [\n",
    "    \"Hello, world!\",\n",
    "    \" Hello\",\n",
    "    \"hello\",\n",
    "    \" hello\",\n",
    "    \"56873+3184623=123456789-1000000000\"  # it's broken\n",
    "]\n",
    "\n",
    "for text in text_li:\n",
    "    token_ids = tokenizer_hf.encode(text)\n",
    "    # cprint(token_ids)\n",
    "    # tokens_text = [tokenizer_hf.convert_ids_to_tokens(id) for id in token_ids]\n",
    "\n",
    "    # # Print tokens alongside their IDs\n",
    "    # for token_id, token_text in zip(token_ids, tokens_text):\n",
    "    #     print(f\"{token_text} (ID: {token_id})\")\n",
    "    cprint(tokenizer_hf.convert_ids_to_tokens(token_ids))\n",
    "\n",
    "    print('=' * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation\n",
    "- Tokenizing the Reference Text: Convert the input text into tokens that the model can understand.\n",
    "- Running the Model with Cache: Generate predictions (logits) and cache the past states for subsequent tokens generation.\n",
    "- Calculating Log Probabilities and Probabilities: For understanding the model's confidence across the vocabulary for the last token.\n",
    "- Decoding Tokens: Convert token IDs back to strings for human-readable text.\n",
    "- Generating the Next Token: Predict the next token based on the current input.\n",
    "- Updating Input with the Next Token: Concatenate the predicted next token with the current input and rerun the model to see the updated predictions.\n",
    "\n",
    "- Multi-step generation: [Generation](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin)\n",
    "```python\n",
    "outputs = model.generate(input_ids, max_length=50, num_beans=5, top_p=0.92)\n",
    "```\n",
    "num_beams=5 indicates beam search with 5 beams, and top_p=0.92 indicates nucleus sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mall_tokens\u001b[0m: \n",
      "tensor([[   40,   716,   281,  4998,  1960,   382, 19741,    11,   875, 12342,\n",
      "            12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,    13,\n",
      "          1881,  1110,   314,   481,  7074,  1692,  1241,  4430,   290,  1011,\n",
      "           625,   262]], device='mps:0')\n",
      "\n",
      "\u001b[93mdecoded_tokens\u001b[0m: \n",
      "['I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure the model is in evaluation mode\n",
    "model_hf.eval()\n",
    "\n",
    "# reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the\"\n",
    "\n",
    "# Tokenize the reference text\n",
    "all_tokens = tokenizer_hf.encode(reference_text, return_tensors=\"pt\").to(device)\n",
    "decoded_tokens = tokenizer_hf.batch_decode(all_tokens, skip_special_tokens=True)\n",
    "\n",
    "cprint(all_tokens)\n",
    "cprint(decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mpred_token_1\u001b[0m: \n",
      "tensor([995], device='mps:0')\n",
      "\n",
      "\u001b[93mtokenizer_hf.decode(pred_token_1)\u001b[0m: \n",
      " world\n",
      "\n",
      "\u001b[93mall_tokens_1\u001b[0m: \n",
      "tensor([[   40,   716,   281,  4998,  1960,   382, 19741,    11,   875, 12342,\n",
      "            12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,    13,\n",
      "          1881,  1110,   314,   481,  7074,  1692,  1241,  4430,   290,  1011,\n",
      "           625,   262,   995]], device='mps:0')\n",
      "\n",
      "\u001b[93mdecoded_tokens_1\u001b[0m: \n",
      "['I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the model with cache, generate new logits\n",
    "with torch.no_grad():\n",
    "    outputs_1 = model_hf(all_tokens, use_cache=True)  # important: use_cache=True\n",
    "    logits_1 = outputs_1.logits\n",
    "\n",
    "# Predict the next token\n",
    "pred_token_1 = logits_1[:, -1, :].argmax(dim=-1)\n",
    "\n",
    "# Update the input with the next token and generate new logits\n",
    "all_tokens_1 = torch.cat([all_tokens, pred_token_1.unsqueeze(-1)], dim=1)\n",
    "decoded_tokens_1 = tokenizer_hf.batch_decode(all_tokens_1, skip_special_tokens=True)\n",
    "\n",
    "cprint(pred_token_1)\n",
    "cprint(tokenizer_hf.decode(pred_token_1))\n",
    "cprint(all_tokens_1)\n",
    "cprint(decoded_tokens_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mpred_token_2\u001b[0m: \n",
      "tensor([13], device='mps:0')\n",
      "\n",
      "\u001b[93mtokenizer_hf.decode(pred_token_2)\u001b[0m: \n",
      ".\n",
      "\n",
      "\u001b[93mall_tokens_2\u001b[0m: \n",
      "tensor([[   40,   716,   281,  4998,  1960,   382, 19741,    11,   875, 12342,\n",
      "            12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,    13,\n",
      "          1881,  1110,   314,   481,  7074,  1692,  1241,  4430,   290,  1011,\n",
      "           625,   262,   995,    13]], device='mps:0')\n",
      "\n",
      "\u001b[93mdecoded_tokens_2\u001b[0m: \n",
      "['I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs_2 = model_hf(all_tokens_1)\n",
    "    logits_2 = outputs_2.logits\n",
    "\n",
    "# Predict the next token\n",
    "pred_token_2 = logits_2[:, -1, :].argmax(-1)\n",
    "\n",
    "# Update the input with the next token and generate new logits\n",
    "all_tokens_2 = torch.cat([all_tokens_1, pred_token_2.unsqueeze(-1)], dim=1)\n",
    "decoded_tokens_2 = tokenizer_hf.batch_decode(all_tokens_2, skip_special_tokens=True)\n",
    "\n",
    "cprint(pred_token_2)\n",
    "cprint(tokenizer_hf.decode(pred_token_2))\n",
    "cprint(all_tokens_2)\n",
    "cprint(decoded_tokens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mall_tokens\u001b[0m: \n",
      "tensor([[   40,   716,   281,  4998,  1960,   382, 19741,    11,   875, 12342,\n",
      "            12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,    13,\n",
      "          1881,  1110,   314,   481,  7074,  1692,  1241,  4430,   290,  1011,\n",
      "           625,   262]], device='mps:0')\n",
      "\n",
      "\u001b[93mall_tokens_1\u001b[0m: \n",
      "tensor([[   40,   716,   281,  4998,  1960,   382, 19741,    11,   875, 12342,\n",
      "            12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,    13,\n",
      "          1881,  1110,   314,   481,  7074,  1692,  1241,  4430,   290,  1011,\n",
      "           625,   262,   995]], device='mps:0')\n",
      "\n",
      "\u001b[93mall_tokens_2\u001b[0m: \n",
      "tensor([[   40,   716,   281,  4998,  1960,   382, 19741,    11,   875, 12342,\n",
      "            12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,    13,\n",
      "          1881,  1110,   314,   481,  7074,  1692,  1241,  4430,   290,  1011,\n",
      "           625,   262,   995,    13]], device='mps:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cprint(all_tokens)\n",
    "cprint(all_tokens_1)\n",
    "cprint(all_tokens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mlogits_1[0, -1, :])  # dim 1 = current text lengt\u001b[0m: \n",
      "tensor([-74.8430, -75.9122, -78.9191,  ..., -84.0591, -80.5990, -76.4612],\n",
      "       device='mps:0')\n",
      "\n",
      "\u001b[93mlogits_1.shape\u001b[0m: \n",
      "torch.Size([1, 32, 50257])\n",
      "\n",
      "\u001b[93mprobs_1[0, -1, :]\u001b[0m: \n",
      "tensor([8.3072e-06, 2.8519e-06, 1.4101e-07,  ..., 8.2602e-10, 2.6282e-08,\n",
      "        1.6470e-06], device='mps:0')\n",
      "\n",
      "\u001b[93mprobs_1.shape\u001b[0m: \n",
      "torch.Size([1, 32, 50257])\n",
      "\n",
      "\u001b[93mlogits_2[0, -1, :])  # dim 1 = current text lengt\u001b[0m: \n",
      "tensor([-60.4925, -65.5512, -70.5624,  ..., -80.5731, -75.2508, -66.8645],\n",
      "       device='mps:0')\n",
      "\n",
      "\u001b[93mlogits_2.shape\u001b[0m: \n",
      "torch.Size([1, 33, 50257])\n",
      "\n",
      "\u001b[93mprobs_2[0, -1, :]\u001b[0m: \n",
      "tensor([4.6065e-02, 2.9271e-04, 1.9502e-06,  ..., 8.7600e-11, 1.7944e-08,\n",
      "        7.8714e-05], device='mps:0')\n",
      "\n",
      "\u001b[93mprobs_2.shape\u001b[0m: \n",
      "torch.Size([1, 33, 50257])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cprint(logits_1[0, -1, :])  # dim 1 = current text length\n",
    "cprint(logits_1.shape)\n",
    "log_probs_1 = torch.nn.functional.log_softmax(logits_1, dim=-1)  # for loss calc\n",
    "probs_1 = torch.nn.functional.softmax(logits_1, dim=-1)\n",
    "cprint(probs_1[0, -1, :])\n",
    "cprint(probs_1.shape)\n",
    "\n",
    "cprint(logits_2[0, -1, :])  # dim 1 = current text length\n",
    "cprint(logits_2.shape)\n",
    "log_probs_2 = torch.nn.functional.log_softmax(logits_2, dim=-1)\n",
    "probs_2 = torch.nn.functional.softmax(logits_2, dim=-1)\n",
    "cprint(probs_2[0, -1, :])\n",
    "cprint(probs_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play Around with Model\n",
    "\n",
    "- Conv1D instead of FC\n",
    "- wte (Word Token Embeddings): This refers to the embedding layer that converts input tokens (words or subwords) into vectors of a fixed size.\n",
    "- wpe (Word Position Embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mmodel_hf\u001b[0m: \n",
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-5): 6 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cprint(model_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mmodel_hf.transformer.h\u001b[0m: \n",
      "ModuleList(\n",
      "  (0-5): 6 x GPT2Block(\n",
      "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (attn): GPT2Attention(\n",
      "      (c_attn): Conv1D()\n",
      "      (c_proj): Conv1D()\n",
      "      (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): GPT2MLP(\n",
      "      (c_fc): Conv1D()\n",
      "      (c_proj): Conv1D()\n",
      "      (act): NewGELUActivation()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cprint(model_hf.transformer.h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_text_2 = \"This is an example.\"\n",
    "\n",
    "# this tokenizes the text, and returns a dictionary with the input_ids and the attention_mask\n",
    "inputs = tokenizer_hf(reference_text_2, return_tensors=\"pt\").to(device)\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "\n",
    "# Disable gradient calculations\n",
    "with torch.no_grad():\n",
    "    # Pass input through the embedding layer\n",
    "    embeddings = model_hf.transformer.wte(input_ids)  # Word Token Embeddings\n",
    "    position_ids = torch.arange(0, input_ids.size(-1), dtype=torch.long, device=input_ids.device)\n",
    "    position_embeddings = model_hf.transformer.wpe(position_ids)  # Word Position Embeddings\n",
    "\n",
    "    # Combine token and position embeddings\n",
    "    hidden_states = embeddings + position_embeddings\n",
    "    hidden_states = model_hf.transformer.drop(hidden_states)  # Apply dropout if it's part of the model\n",
    "\n",
    "    # Pass through the first layer normalization if needed\n",
    "    hidden_states = model_hf.transformer.h[0].ln_1(hidden_states)\n",
    "\n",
    "    # Now, hidden_states is the input to the first attention layer\n",
    "    input_to_att = hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93minputs\u001b[0m: \n",
      "{'input_ids': tensor([[1212,  318,  281, 1672,   13]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1]], device='mps:0')}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cprint(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93membeddings\u001b[0m: \n",
      "tensor([[[ 0.0254, -0.1193,  0.1040,  ...,  0.0850, -0.0361,  0.1535],\n",
      "         [-0.0006,  0.0075,  0.0307,  ...,  0.1909, -0.0206,  0.0218],\n",
      "         [-0.1129, -0.0073,  0.0532,  ...,  0.0279,  0.0783, -0.1056],\n",
      "         [ 0.0488, -0.1293,  0.0764,  ..., -0.4437, -0.0842, -0.1168],\n",
      "         [ 0.0400, -0.0202,  0.0025,  ..., -0.0923,  0.0308,  0.1553]]],\n",
      "       device='mps:0')\n",
      "\n",
      "\u001b[93membeddings.shape\u001b[0m: \n",
      "torch.Size([1, 5, 768])\n",
      "\n",
      "\u001b[93mposition_ids\u001b[0m: \n",
      "tensor([0, 1, 2, 3, 4], device='mps:0')\n",
      "\n",
      "\u001b[93mposition_embeddings\u001b[0m: \n",
      "tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
      "          2.8267e-02,  5.4490e-02],\n",
      "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
      "          1.0172e-02, -1.5573e-04],\n",
      "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
      "          1.9325e-02, -2.1424e-02],\n",
      "        [-2.8337e-04, -7.3803e-02,  1.0553e-01,  ...,  1.0157e-02,\n",
      "          1.7659e-02, -7.0854e-03],\n",
      "        [ 7.6374e-03, -2.5090e-02,  1.2696e-01,  ...,  8.4643e-03,\n",
      "          9.8542e-03, -7.0117e-03]], device='mps:0')\n",
      "\n",
      "\u001b[93mposition_embeddings.shape\u001b[0m: \n",
      "torch.Size([5, 768])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cprint(embeddings)\n",
    "cprint(embeddings.shape)\n",
    "\n",
    "cprint(position_ids)\n",
    "cprint(position_embeddings)\n",
    "cprint(position_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    att_output = model_hf.transformer.h[0](input_to_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93minput_to_att\u001b[0m: \n",
      "tensor([[[ 0.0137, -0.1418,  0.0306,  ...,  0.0233, -0.0181,  0.1000],\n",
      "         [ 0.0328, -0.0246, -0.0642,  ...,  0.1969, -0.0240,  0.0149],\n",
      "         [-0.1133, -0.0709,  0.0705,  ...,  0.0463,  0.0701, -0.1224],\n",
      "         [ 0.0596, -0.1657,  0.1214,  ..., -0.3813, -0.0705, -0.1132],\n",
      "         [ 0.0632, -0.0298,  0.0914,  ..., -0.0815,  0.0206,  0.1397]]],\n",
      "       device='mps:0')\n",
      "\n",
      "\u001b[93minput_to_att.shape\u001b[0m: \n",
      "torch.Size([1, 5, 768])\n",
      "\n",
      "\u001b[93matt_output\u001b[0m: \n",
      "(tensor([[[ 1.5787e+00, -1.2246e+00,  1.9400e+00,  ..., -2.0778e+00,\n",
      "           1.7360e-01,  5.4510e-01],\n",
      "         [-1.4459e+00, -2.4165e+00,  1.9684e+00,  ..., -4.9854e-01,\n",
      "           8.1772e-01,  1.1241e+00],\n",
      "         [-6.5890e-01, -9.0903e-01,  3.4541e-01,  ..., -5.7806e-01,\n",
      "           9.1351e-01, -5.6571e-01],\n",
      "         [-9.1027e-01, -1.0106e+00, -2.3466e-01,  ..., -2.7518e+00,\n",
      "           4.9475e-01,  9.5350e-04],\n",
      "         [-4.1520e-01, -1.9565e+00, -2.6256e-01,  ..., -7.5900e-01,\n",
      "           9.6207e-01,  3.8390e-02]]], device='mps:0'),)\n",
      "\n",
      "\u001b[93matt_output[0].shape\u001b[0m: \n",
      "torch.Size([1, 5, 768])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cprint(input_to_att)\n",
    "cprint(input_to_att.shape)\n",
    "\n",
    "cprint(att_output)\n",
    "cprint(att_output[0].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
