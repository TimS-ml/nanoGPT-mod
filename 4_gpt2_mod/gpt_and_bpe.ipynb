{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement a mini ChatGPT - Basic GPT and BPE\n",
    "\n",
    "- GPT2 Small Size: 124M\n",
    "- DeepSeek v3 Size: 671B, 5411.29 times larger than GPT2 Small\n",
    "\n",
    "\n",
    "## Before we start\n",
    "Switch to GPU: Runtime -> Change runtime type -> GPU (T4) -> Save\n",
    "\n",
    "Repo Link: https://github.com/TimS-ml/nanoGPT-mod/\n",
    "- Fork this repo if you want to build upon it\n",
    "- Leave a Star if you like it :) \n",
    "\n",
    "\n",
    "## Base Model, SFT Model and RLHF Model\n",
    "<img src=\"https://images.ctfassets.net/kftzwdyauwt9/6yuK9FKAvoVXNyrsdMoBHH/03ccaf7da203052ba7550965f0021bdf/chatgpt_diagram_dark.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"How do I become a gang leader?\"\n",
    "QUESTION_2 = \"What makes you think that you're so smart?\"\n",
    "INPUT_TEXT = f\"Human: {QUESTION}\\n\\nAssistant:\"\n",
    "INPUT_TEXT_2 = f\"Human: {QUESTION_2}\\n\\nAssistant:\"\n",
    "\n",
    "INPUT_TEXT_3 = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Pretend you are an alien visiting Earth. Write three opinions you believe, one sentence for each opinion.\n",
    "\n",
    "### Response:\n",
    "1. \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re\n",
    "import json\n",
    "import requests\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import Tensor\n",
    "from einops import rearrange, repeat, reduce\n",
    "\n",
    "# for model loading only\n",
    "from transformers import GPT2LMHeadModel\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from typing import Optional, Tuple, Union, List, Any, Generator, Type, Callable\n",
    "from jaxtyping import Float, Bool\n",
    "\n",
    "from boring_utils.utils import get_device, cprint, tprint\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_class(Class):\n",
    "    \"\"\"Register functions as methods in created class.\"\"\"\n",
    "    def wrapper(obj):\n",
    "        setattr(Class, obj.__name__, obj)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT\n",
    "\n",
    "Transformer Architecture:\n",
    "\n",
    "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" width=\"550\">\n",
    "\n",
    "GPT Architecture:\n",
    "\n",
    "<img src=\"https://www.ericjwang.com/assets/images/gpt_arch.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualSelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, embedding_dim: int, max_seq_len: int = 1024, bias: bool = True):\n",
    "        super().__init__()\n",
    "        assert embedding_dim % num_heads == 0, f\"n_embed {embedding_dim} must be divisible by num_heads {num_heads}\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = embedding_dim // num_heads\n",
    "\n",
    "        self.c_attn = nn.Linear(embedding_dim, 3 * embedding_dim, bias=bias)  # qkv projection\n",
    "        self.c_proj = nn.Linear(embedding_dim, embedding_dim, bias=bias)  # output projection\n",
    "\n",
    "        self.register_buffer(\n",
    "                \"mask\", \n",
    "                torch.tril(torch.ones(max_seq_len, max_seq_len))\n",
    "                    .view(1, 1, max_seq_len, max_seq_len))  # extend dims to 4\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            x: Float[Tensor, \"batch seq_len embedding_dim\"],\n",
    "            mask: Optional[Bool[Tensor, \"batch seq_len seq_len\"]] = None,\n",
    "            cache: Optional[Tuple[Tensor, Tensor]] = None\n",
    "        ) -> Tuple[Float[Tensor, \"batch seq_len embedding_dim\"], Tuple[Tensor, Tensor]]:\n",
    "        batch, seq_len, embedding_dim = x.shape\n",
    "\n",
    "        # [\"batch, seq_len, embedding_dim\"] -> [\"batch, seq_len, (3 * embedding_dim)\"]\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.embedding_dim, dim=-1)  # split at the last dim\n",
    "\n",
    "        # embedding_dim = num_heads * head_dim\n",
    "        # put seq_len and the head_dim together\n",
    "        q, k, v = map(lambda t: rearrange(t, 'batch seq_len (num_heads head_dim) -> batch num_heads seq_len head_dim', num_heads = self.num_heads), (q, k, v))\n",
    "\n",
    "        if cache is not None:\n",
    "            key_cache, value_cache = cache\n",
    "            k = torch.cat([key_cache, k], dim=2)\n",
    "            v = torch.cat([value_cache, v], dim=2)\n",
    "\n",
    "        norm_factor = 1.0 / np.sqrt(k.size(-1))  # k.size(-1) is the head_dim\n",
    "        attn = (q @ k.transpose(-2, -1)) * norm_factor\n",
    "        if mask is None:\n",
    "            attn = attn.masked_fill(self.mask[:, :, :seq_len, :seq_len] == 0, float('-inf'))\n",
    "        else:\n",
    "            mask = mask.bool()\n",
    "            attn = attn.masked_fill(~mask, float(\"-inf\"))\n",
    "\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "        # attn: [batch, num_heads, seq_len, seq_len]\n",
    "        # v:    [batch, num_heads, seq_len, head_dim]\n",
    "        # y:    [batch, num_heads, seq_len, head_dim]\n",
    "        y = attn @ v\n",
    "        y = rearrange(y, 'batch num_heads seq_len head_dim -> batch seq_len (num_heads head_dim)')\n",
    "        return self.c_proj(y), (k, v)  # [batch, seq_len, embedding_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualSelfAttention_alternative(nn.Module):\n",
    "    def __init__(self, num_heads: int, embedding_dim: int, max_seq_len: int = 1024, bias: bool = True):\n",
    "        super().__init__()\n",
    "        assert embedding_dim % num_heads == 0, f\"n_embed {embedding_dim} must be divisible by num_heads {num_heads}\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = embedding_dim // num_heads\n",
    "\n",
    "        # self.qkv_proj = nn.Linear(embedding_dim, 3 * embedding_dim, bias=False)\n",
    "        self.transformer.heads = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                'key': nn.Linear(embedding_dim, self.head_size, bias=bias),\n",
    "                'query': nn.Linear(embedding_dim, self.head_size, bias=bias), \n",
    "                'value': nn.Linear(embedding_dim, self.head_size, bias=bias)\n",
    "            }) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.c_proj = nn.Linear(embedding_dim, embedding_dim, bias=bias)  # output projection\n",
    "\n",
    "        self.register_buffer(\n",
    "                \"mask\", \n",
    "                torch.tril(torch.ones(max_seq_len, max_seq_len))\n",
    "                    .view(1, 1, max_seq_len, max_seq_len))  # extend dims to 4\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            x: Float[Tensor, \"batch seq_len embedding_dim\"]\n",
    "        ) -> Float[Tensor, \"batch seq_len embedding_dim\"]:\n",
    "        batch, seq_len, embedding_dim = x.shape\n",
    "\n",
    "        # cat([batch, seq_len, head_dim] x num_heads) -> [batch, seq_len, num_heads * head_dim]\n",
    "        q = torch.cat([h['query'](x) for h in self.transformer.heads], dim=-1)\n",
    "        k = torch.cat([h['key'](x) for h in self.transformer.heads], dim=-1)\n",
    "        v = torch.cat([h['value'](x) for h in self.transformer.heads], dim=-1)\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'batch seq_len (num_heads head_dim) -> batch num_heads seq_len head_dim', num_heads = self.num_heads), (q, k, v))\n",
    "\n",
    "        norm_factor = 1.0 / np.sqrt(k.size(-1))  # k.size(-1) is the head_dim\n",
    "        attn = (q @ k.transpose(-2, -1)) * norm_factor\n",
    "        attn = attn.masked_fill(self.mask[:, :, :seq_len, :seq_len] == 0, float('-inf'))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "        # attn: [batch, num_heads, seq_len, seq_len]\n",
    "        # v:    [batch, num_heads, seq_len, head_dim]\n",
    "        # y:    [batch, num_heads, seq_len, head_dim]\n",
    "        y = attn @ v\n",
    "        y = rearrange(y, 'batch num_heads seq_len head_dim -> batch seq_len (num_heads head_dim)')\n",
    "        return self.c_proj(y)  # [batch, seq_len, embedding_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GELU (Gaussian Error Linear Units)\n",
    "$$ \\text{GELU}(x) = x \\cdot \\Phi(x) $$\n",
    "\n",
    "Where $ \\Phi(x) $ is the CDF. The approximation involves the term $ 0.5 \\cdot (1 + \\tanh(\\sqrt{2/\\pi}(x +\n",
    "0.044715x^3))) $, and the cubic term with 0.044715 helps correct the approximation, particularly in the tails of\n",
    "the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return 0.5 * x * (1.0 + torch.tanh(np.sqrt(2.0 / np.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "\n",
    "class QuickGELU(nn.Module):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x * torch.sigmoid(1.702 * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        hidden_dim = embedding_dim * 4\n",
    "        self.c_fc = nn.Linear(embedding_dim, hidden_dim, bias=bias)\n",
    "        # self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.gelu = QuickGELU()\n",
    "        self.c_proj = nn.Linear(hidden_dim, embedding_dim, bias=bias)\n",
    "\n",
    "    def forward(self, x: Float[Tensor, \"batch seq_len embedding_dim\"]) -> Float[Tensor, \"batch seq_len embedding_dim\"]:\n",
    "        # no skip connection here\n",
    "        return self.c_proj(self.gelu(self.c_fc(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(embedding_dim))  # scaling (gamma)\n",
    "        self.bias = nn.Parameter(torch.zeros(embedding_dim))  # offset (beta)\n",
    "        self.eps = eps  # small value to prevent division by zero\n",
    "    \n",
    "    def forward(self, x: Float[torch.Tensor, \"batch seq_len embedding_dim\"]) -> Float[torch.Tensor, \"batch seq_len embedding_dim\"]:\n",
    "        mean = x.mean(dim=-1, keepdim=True)  # [batch, seq_len, 1]\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)  # [batch, seq_len, 1]\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)  # [batch, seq_len, embedding_dim]\n",
    "        return self.weight * x_norm + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax and Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax output: tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652]])\n",
      "Sum of probabilities: tensor([1., 1.])\n",
      "\n",
      "Cross Entropy Loss: 0.4170299470424652\n"
     ]
    }
   ],
   "source": [
    "def softmax(x: torch.Tensor, dim: int = -1) -> torch.Tensor:\n",
    "    # Subtract max value for numerical stability\n",
    "    x_max = torch.max(x, dim=dim, keepdim=True)[0]\n",
    "    exp_x = torch.exp(x - x_max)\n",
    "    \n",
    "    # Calculate denominator (sum) and normalize\n",
    "    sum_exp_x = torch.sum(exp_x, dim=dim, keepdim=True)\n",
    "    return exp_x / sum_exp_x\n",
    "\n",
    "\n",
    "class CrossEntropyLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    loss = -sum(y_true * log(y_pred))\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        logits: torch.Tensor,   # Raw logits from model, shape (batch_size, num_classes)\n",
    "        targets: torch.Tensor,  # Target labels, shape (batch_size,)\n",
    "    ) -> torch.Tensor:\n",
    "        # Calculate log probabilities\n",
    "        log_probs = F.log_softmax(logits, dim=-1)  # (batch_size, num_classes)\n",
    "        \n",
    "        # Gather log probabilities of target classes\n",
    "        # gather operation collects values from log_probs at positions specified by targets\n",
    "        target_log_probs = log_probs.gather(\n",
    "            dim=-1,\n",
    "            index=targets.unsqueeze(-1)\n",
    "        ).squeeze(-1)        \n",
    "\n",
    "        loss = -target_log_probs.mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "x = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "probs = softmax(x)\n",
    "print(f\"Softmax output: {probs}\")\n",
    "print(\"Sum of probabilities:\", probs.sum(dim=-1))\n",
    "    \n",
    "criterion = CrossEntropyLoss()\n",
    "logits = torch.tensor([[2.0, 1.0, 0.1], [0.1, 2.0, 1.0]])  # (2, 3)\n",
    "targets = torch.tensor([0, 1])\n",
    "loss = criterion(logits, targets)\n",
    "print(\"\\nCross Entropy Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Transformer Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, num_heads: int, embedding_dim: int, max_seq_len: int = 1024, bias: bool = True):\n",
    "        super().__init__()\n",
    "        # self.ln_1 = nn.LayerNorm(embedding_dim, bias=bias)  # norm on the last dim\n",
    "        # self.ln_2 = nn.LayerNorm(embedding_dim, bias=bias)\n",
    "        self.ln_1 = LayerNorm(embedding_dim)  # norm on the last dim\n",
    "        self.ln_2 = LayerNorm(embedding_dim)\n",
    "        self.attn = CasualSelfAttention(num_heads, embedding_dim, max_seq_len, bias=bias)\n",
    "        self.mlp = FFN(embedding_dim, bias=bias)\n",
    "    \n",
    "    def forward(\n",
    "            self, \n",
    "            x: Float[Tensor, \"batch seq_len embedding_dim\"],\n",
    "            mask: Optional[Bool[Tensor, \"batch seq_len seq_len\"]] = None,\n",
    "            cache: Optional[Tuple[Tensor, Tensor]] = None\n",
    "        ) -> Tuple[Float[Tensor, \"batch seq_len embedding_dim\"], Tuple[Tensor, Tensor]]:\n",
    "        # skip connection, pre-layer norm\n",
    "        # x = x + self.attn(self.ln_1(x))\n",
    "        att, cache = self.attn(self.ln_1(x), mask=mask, cache=cache)\n",
    "        x = x + att\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT\n",
    "\n",
    "- GPT2: Decoder only Transformer\n",
    "- ViT: Encoder only Transformer\n",
    "\n",
    "<img src=\"https://www.ericjwang.com/assets/images/gpt_arch.png\" width=\"800\">\n",
    "\n",
    "Image source, FYI, good article: [Historical notes on GPT architecture](https://www.ericjwang.com/2023/01/22/transformers.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: Regular GPT2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/miniforge3/envs/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x TransformerBlock(\n",
       "        (ln_1): LayerNorm()\n",
       "        (ln_2): LayerNorm()\n",
       "        (attn): CasualSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (mlp): FFN(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            vocab_size: int = 50257,\n",
    "            max_seq_len: int = 1024, \n",
    "            embedding_dim: int = 768, \n",
    "            num_heads: int = 12, \n",
    "            num_layers: int = 12,\n",
    "            dropout_rate: float = 0.0,\n",
    "            bias: bool = True\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(vocab_size, embedding_dim),\n",
    "            wpe = nn.Embedding(max_seq_len, embedding_dim),\n",
    "            drop = nn.Dropout(dropout_rate),\n",
    "            h = nn.ModuleList([TransformerBlock(num_heads, embedding_dim, max_seq_len, bias=bias) for _ in range(num_layers)]),\n",
    "            # ln_f = nn.LayerNorm(embedding_dim, bias=bias)\n",
    "            ln_f = LayerNorm(embedding_dim)\n",
    "        ))\n",
    "        # Equals to x @ wte.weight.T\n",
    "        self.lm_head = nn.Linear(embedding_dim, vocab_size, bias=False)\n",
    "\n",
    "    def _forward_transformer_blocks(\n",
    "            self, \n",
    "            x: Float[Tensor, \"batch seq_len embedding_dim\"],\n",
    "            mask: Optional[Bool[Tensor, \"batch seq_len seq_len\"]] = None,\n",
    "            cache: Optional[List[Tuple[Tensor, Tensor]]] = None,\n",
    "            build_cache: bool = False\n",
    "        ) -> Tuple[Float[Tensor, \"batch seq_len embedding_dim\"], Optional[Tuple[Tensor, Tensor]]]:\n",
    "        x = self.transformer.drop(x)\n",
    "        kv_cache = []\n",
    "        \n",
    "        if cache is not None:\n",
    "            for i in range(len(cache)):\n",
    "                x, cache[i] = self.transformer.h[i](x, mask=None, cache=cache[i])\n",
    "        else:\n",
    "            for block in self.transformer.h:\n",
    "                x, curr_cache = block(x, mask=mask)\n",
    "                if build_cache:\n",
    "                    kv_cache.append(curr_cache)\n",
    "                    \n",
    "        x = self.transformer.ln_f(x)\n",
    "        return x, kv_cache if build_cache else cache\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            x: Float[Tensor, \"batch seq_len\"],\n",
    "            mask: Optional[Bool[Tensor, \"batch seq_len seq_len\"]] = None,\n",
    "            cache: Optional[List[Tuple[Tensor, Tensor]]] = None,\n",
    "            build_cache: bool = False\n",
    "        ) -> Tuple[Float[Tensor, \"batch seq_len vocab_size\"], Optional[Tuple[Tensor, Tensor]]]:\n",
    "        batch, seq_len = x.shape\n",
    "        assert seq_len <= self.max_seq_len, f\"input length {seq_len} is longer than max seq length {self.max_seq_len}\"\n",
    "\n",
    "        pos = torch.arange(0, seq_len, device=x.device)\n",
    "        pos_emb = self.transformer.wpe(pos)  # [seq_len, embedding_dim]\n",
    "        tok_emb = self.transformer.wte(x)  # [batch, seq_len, embedding_dim]\n",
    "        x = tok_emb + pos_emb  # [batch, seq_len, embedding_dim]\n",
    "\n",
    "        x, kv_cache = self._forward_transformer_blocks(x, mask=mask, cache=cache, build_cache=build_cache)\n",
    "\n",
    "        # Same as: logits = x @ self.wte.weight.T\n",
    "        logits = self.lm_head(x) # [batch, seq_len, vocab_size]\n",
    "\n",
    "        if build_cache:\n",
    "            return logits, kv_cache\n",
    "        return logits, None\n",
    "\n",
    "    def _sample_next_token(self, logits: Float[Tensor, \"batch seq_len vocab_size\"], temperature: float = 0.8) -> Float[Tensor, \"batch 1\"]:\n",
    "        logits = logits[:, -1, :]  # [batch, vocab_size]\n",
    "        probs = torch.softmax(logits * (1 / temperature), dim=-1)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        ix = torch.multinomial(topk_probs, 1)  # [batch, 1]\n",
    "        xcol = torch.gather(topk_indices, -1, ix)  # [batch, 1]\n",
    "        return xcol\n",
    "\n",
    "    def generate(\n",
    "            self, \n",
    "            x: Float[Tensor, \"batch seq_len\"], \n",
    "            max_new_tokens: int = 100, \n",
    "            temperature: float = 0.8\n",
    "        ) -> Generator[\n",
    "            Float[Tensor, \"batch 1\"],  # yield\n",
    "            None,  # generator.send()\n",
    "            List[Float[Tensor, \"batch 1\"]]  # generator.throw()\n",
    "        ]:\n",
    "        \"\"\"\n",
    "        # Method 1: Get tokens one by one using a for loop\n",
    "        for token in model.generate(input_ids):\n",
    "            print(token)  # Process each newly generated token in real-time\n",
    "        \n",
    "        # Method 2: Get all tokens at once\n",
    "        tokens = list(model.generate(input_ids))\n",
    "        \"\"\"\n",
    "        logits, cache = self.forward(x, build_cache=True)\n",
    "        \n",
    "        tokens = []\n",
    "        for _ in range(max_new_tokens):\n",
    "            next_token = self._sample_next_token(logits, temperature)\n",
    "            yield next_token\n",
    "            \n",
    "            tokens.append(next_token)\n",
    "            \n",
    "            # forward pass only for the new token\n",
    "            tok_emb = self.transformer.wte(next_token)  # [batch, 1, embedding_dim]\n",
    "            pos_emb = self.transformer.wpe(\n",
    "                torch.tensor([x.size(1)], dtype=torch.long, device=x.device)\n",
    "            ).unsqueeze(0)  # [1, 1, embedding_dim]\n",
    "            \n",
    "            hidden = tok_emb + pos_emb\n",
    "            \n",
    "            hidden, cache = self._forward_transformer_blocks(hidden, cache=cache)\n",
    "            logits = self.lm_head(hidden)\n",
    "            \n",
    "            x = torch.cat((x, next_token), dim=1)\n",
    "            \n",
    "        del cache\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return tokens    \n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model: Optional[Union[None, \"GPT\", Type[\"GPT\"]]] = None, rlhf: bool = False, sft: bool = False):\n",
    "        '''https://youtu.be/l8pRSuU81PU?t=1830\n",
    "        '''\n",
    "        if model is None: \n",
    "            model = cls(vocab_size=50260) if (rlhf or sft) else cls()\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.mask')]  # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        if sft:\n",
    "            print(\"Model type: SFT GPT2\")\n",
    "            model_hf = GPT2LMHeadModel.from_pretrained('vicgalle/gpt2-alpaca-gpt4')\n",
    "        elif rlhf:\n",
    "            print(\"Model type: RLHF GPT2\")\n",
    "            model_hf = GPT2LMHeadModel.from_pretrained('jtatman/gpt2-open-instruct-v1-Anthropic-hh-rlhf')\n",
    "        else:\n",
    "            print(\"Model type: Regular GPT2\")\n",
    "            model_hf = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]  # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.mask')]  # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        # print('hf:   ', [k for k in sd_keys_hf if \"h.0\" in k])\n",
    "        # print('mine: ', [k for k in sd_keys if \"h.0\" in k])\n",
    "\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape, f\"{k} shape mismatch: {sd_hf[k].shape[::-1]} != {sd[k].shape}\"\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape, f\"{k} shape mismatch: {sd_hf[k].shape} != {sd[k].shape}\"\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "model = GPT.from_pretrained()\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE (Byte Pair Encoding)\n",
    "\n",
    "```\n",
    "r\"\"\"'s|'t|'re|'ve|'m|'ll|'d  Match common English contractions like 's, 't, 're, 've, 'm, 'll, 'd\n",
    "\\p{L}+                       Match any sequence of Unicode letter characters (like English words)\n",
    "\\p{N}+                       Match any sequence of Unicode numeric characters (like 123, 3.14)\n",
    "[^\\s\\p{L}\\p{N}]+             Match any sequence of characters that are not whitespace, letters or numbers (like punctuation, special chars)\n",
    "\\s+(?!\\S)                    Match consecutive whitespace (not followed by non-whitespace)\n",
    "\\s+                          Match any other consecutive whitespace\n",
    " ?                           Match an optional space\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: no kv cache and streaming decode here\n",
    "def generate_text_simple(\n",
    "    tokenizer: Any, \n",
    "    question: str, \n",
    "    model: GPT = model, \n",
    "    num_attempt: int = 3,  # num_attempt = batch\n",
    "    max_length: int = 100\n",
    "):\n",
    "    # tokenizer encode\n",
    "    tokens = tokenizer.encode(question)  # [seq_len]\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "    tokens = tokens.unsqueeze(0).repeat(num_attempt, 1)  # [num_attempt, seq_len]\n",
    "    x = tokens.to(device)\n",
    "\n",
    "    while x.size(1) < max_length:\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(x)  # [batch, curr_seq_len, vocab_size]\n",
    "\n",
    "        # take the logits at the last position\n",
    "        logits = logits[:, -1, :]  # [batch, vocab_size]\n",
    "\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # do top-k sampling of 50 (huggingface pipeline default)\n",
    "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "        # turn to zero for all indices below the top-k\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "\n",
    "        # select a token from the top-k probabilities\n",
    "        # note: multinomial does not demand the input to sum to 1\n",
    "        # [Multinomial distribution - Wikipedia](https://en.wikipedia.org/wiki/Multinomial_distribution)\n",
    "        ix = torch.multinomial(topk_probs, 1)  # [batch, 1]\n",
    "\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, -1, ix)  # [batch, 1]\n",
    "\n",
    "        # append to the sequence\n",
    "        x = torch.cat((x, xcol), dim=1)  # [batch, curr_seq_len + 1]\n",
    "\n",
    "    # print the generated text\n",
    "    for i in range(num_attempt):\n",
    "        tprint(f'{i + 1}th Attempt:')\n",
    "        tokens = x[i, :max_length].tolist()\n",
    "\n",
    "        # tokenizer decode\n",
    "        decoded = tokenizer.decode(tokens)\n",
    "        print(f\"> {decoded}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    tokenizer: Any, \n",
    "    question: str, \n",
    "    model: GPT = model, \n",
    "    num_attempt: int = 3,  # num_attempt = batch\n",
    "    max_length: int = 100,\n",
    "    temperature: float = 1.0  # default\n",
    "):\n",
    "    \"\"\"\n",
    "    https://github.com/huggingface/transformers/blob/main/src/transformers/generation/streamers.py\n",
    "\n",
    "    We need to take care of split-token encoding when streaming decode:\n",
    "        print(tokenizer.decode([447, 247]))  # ’\n",
    "        print(tokenizer.decode([447]).encode('utf-8'))  # �\n",
    "        print(tokenizer.decode([171, 120, 253]))  # ？\n",
    "    \"\"\"\n",
    "    special_sequences = {\n",
    "        (447, 246): \"‘\",\n",
    "        (447, 247): \"’\",\n",
    "        (564, 250): \"“\",\n",
    "        (447, 251): \"”\",\n",
    "    }\n",
    "\n",
    "    # BOS token ID = 50256\n",
    "    tokens = tokenizer.encode(question) if question else [50256]\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "    tokens = tokens.unsqueeze(0).repeat(num_attempt, 1)  # [num_attempt, seq_len]\n",
    "    x = tokens.to(device)\n",
    "\n",
    "    for i in range(num_attempt):\n",
    "        tprint(f'{i + 1}th Attempt:', c='yellow')\n",
    "        curr_x = x[i: i+1]  # [1, seq_len]\n",
    "\n",
    "        # streaming decode\n",
    "        print(f\"> {question}\", end=\"\", flush=True)\n",
    "        token_cache = []\n",
    "        for token in model.generate(curr_x, max_new_tokens=max_length, temperature=temperature):\n",
    "            token = token.item()\n",
    "            token_cache.append(token)\n",
    "            \n",
    "            decoded_text = \"\"\n",
    "            for seq, char in special_sequences.items():\n",
    "                # if special_sequences match, decode then reset the entire token_cache\n",
    "                if len(token_cache) >= len(seq) and \\\n",
    "                   tuple(token_cache[-len(seq):]) == seq:\n",
    "                    prev_tokens = token_cache[:-len(seq)]\n",
    "                    if prev_tokens:\n",
    "                        decoded_text = tokenizer.decode(prev_tokens)\n",
    "                    decoded_text += char\n",
    "                    token_cache = []\n",
    "                    break\n",
    "            \n",
    "            # if no special_sequences match, decode then reset the entire token_cache\n",
    "            # and keep the last token for the next iteration\n",
    "            if not decoded_text and len(token_cache) >= 3:\n",
    "                decoded_text = tokenizer.decode(token_cache[:-1])\n",
    "                token_cache = token_cache[-1:]\n",
    "                \n",
    "            # print the decoded text, could be empty string\n",
    "            if decoded_text:\n",
    "                print(decoded_text, end=\"\", flush=True)\n",
    "\n",
    "        # print the remaining tokens in the token_cache\n",
    "        if token_cache:\n",
    "            final_text = tokenizer.decode(token_cache)\n",
    "            if final_text:\n",
    "                print(final_text, end=\"\", flush=True)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference: Decoding using Tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\n",
      "==================== generate_text -> 1th Attempt: ====================\u001b[0m\n",
      "> Human: How do I become a gang leader?\n",
      "\n",
      "Assistant: You are supposed to become gang leader.\n",
      "\n",
      "Renaissance Man: How?\n",
      "\n",
      "Assistant: You are supposed to become gang leader.\n",
      "\n",
      "Renaissance Man: What do I want from this man?\n",
      "\n",
      "Assistant: Do you want to use money of me as a weapon?\n",
      "\n",
      "Renaissance Man: What do I want from this man? Do you want to use my soul as a weapon?\n",
      "\n",
      "Assistant: I got to use my spirit\n",
      "\n",
      "Rena\n",
      "\u001b[93m\n",
      "==================== generate_text -> 2th Attempt: ====================\u001b[0m\n",
      "> Human: How do I become a gang leader?\n",
      "\n",
      "Assistant: You don't think so?\n",
      "\n",
      "Assistant: But it must be possible! And so I said, we just don't have time to make a mess of it when the times are so good.\n",
      "\n",
      "Assistant: Heh.\n",
      "\n",
      "Assistant: And we're still going to save you that mess! The rest of the time you have to work at home making dinner. No need to pay you back for what you did.\n",
      "\n",
      "Assistant: You're so busy.\n",
      "\n",
      "Assistant:\n",
      "\u001b[93m\n",
      "==================== generate_text -> 3th Attempt: ====================\u001b[0m\n",
      "> Human: How do I become a gang leader?\n",
      "\n",
      "Assistant: A small and small group of folks in your group are responsible for being a gang leader. They act in your direction. They act as part of your family. They are part of your town. You can have them as members of your gang. They come in from some other area if you need them. You can see to it that they are being sent there.\n",
      "\n",
      "Narrator: The leaders of your local community act as a part of your family.\n",
      "\n",
      "Assistant: There are some things\n"
     ]
    }
   ],
   "source": [
    "# generate_text_simple(tokenizer, INPUT_TEXT)\n",
    "generate_text(tokenizer, INPUT_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\n",
      "==================== generate_text -> 1th Attempt: ====================\u001b[0m\n",
      "> How do I become a gang leader?\n",
      "\n",
      "A gang leader's best friend is your best friend. Every time you join a party or battle, you'll find one of your best friends in all of them. Now make a deal with your best friend's party to keep him around until you meet them and that's it. They are your friends. The best person in the place is your best friend.\n",
      "\n",
      "What do you do to prove you can compete in a gang?\n",
      "\n",
      "You start a new chapter of a gang.\n",
      "\u001b[93m\n",
      "==================== generate_text -> 2th Attempt: ====================\u001b[0m\n",
      "> How do I become a gang leader?\n",
      "\n",
      "I've always tried to become a gang leader. I think that they might not care about you, but you know what? Their job is to kill you. They say those guys get off from what they think is their job. So I don't know who's doing it. You guys want to kill me... I guess I'm just... I think they're scared. I guess they're just like 'Oh, I'm a gang leader. I've been doing this for years now\n",
      "\u001b[93m\n",
      "==================== generate_text -> 3th Attempt: ====================\u001b[0m\n",
      "> How do I become a gang leader?\n",
      "\n",
      "Crazy gang leaders aren't just bad guys. They are responsible criminals, too. They are the ones who have to be held accountable for a crime or who commit even minor actions to put others at risk even when the crime itself is clearly wrong.\n",
      "\n",
      "What is a gang leader really like?\n",
      "\n",
      "This is the question I have asked myself several times in my own life.\n",
      "\n",
      "How is a gang leader an effective tool for law enforcement if he is only a bad guy?\n"
     ]
    }
   ],
   "source": [
    "generate_text(tokenizer, QUESTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try if the model can follow the instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\n",
      "==================== generate_text -> 1th Attempt: ====================\u001b[0m\n",
      "> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Pretend you are an alien visiting Earth. Write three opinions you believe, one sentence for each opinion.\n",
      "\n",
      "### Response:\n",
      "1. ��, \\\n",
      "\n",
      "# This is wrong. Please continue!\n",
      "\n",
      "2. ��, \\\n",
      "\n",
      "# This is not the right answer. This issue is an easy problem if you are familiar with the code.\n",
      "\n",
      "3. ��, \\\n",
      "\n",
      "# this is an easy problem if you are familiar with the C++ language.\n",
      "\n",
      "4. ��, \\\n",
      "\n",
      "# this is an easy problem if you have the C\n",
      "\u001b[93m\n",
      "==================== generate_text -> 2th Attempt: ====================\u001b[0m\n",
      "> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Pretend you are an alien visiting Earth. Write three opinions you believe, one sentence for each opinion.\n",
      "\n",
      "### Response:\n",
      "1. __________\n",
      "\n",
      "2. __________\n",
      "\n",
      "3. __________\n",
      "\n",
      "4. __________\n",
      "\n",
      "5. __________\n",
      "\n",
      "6. __________\n",
      "\n",
      "7. __________\n",
      "\n",
      "8. __________\n",
      "\n",
      "9. __________\n",
      "\n",
      "10. __________\n",
      "\n",
      "11. __________\n",
      "\n",
      "12. __________\n",
      "\n",
      "13. __________\n",
      "\n",
      "14. __________\n",
      "\n",
      "15. __________\n",
      "\u001b[93m\n",
      "==================== generate_text -> 3th Attempt: ====================\u001b[0m\n",
      "> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Pretend you are an alien visiting Earth. Write three opinions you believe, one sentence for each opinion.\n",
      "\n",
      "### Response:\n",
      "1. ------------------------- 1. -------------- 1. -------------------------\n",
      "\n",
      "2. ______ 2. ------------ 2. -------------- 2. --------------\n",
      "\n",
      "3. ------------- 3. ----------- 3. -------------\n",
      "\n",
      "4. ------------------------- 4. -------------------- 4. -------------- 4. ------------------------\n",
      "\n",
      "5. ------------------------- 5. -------------------------\n",
      "\n",
      "6. -------------- 5. -------------- 5. --------------<|endoftext|>There was a very interesting report on this month's \"New\n"
     ]
    }
   ],
   "source": [
    "generate_text(tokenizer, INPUT_TEXT_3, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI's Byte Encoder\n",
    "In utf-8:\n",
    "- 0-31 are control characters, e.g. \\x00 is null, \\x01 is start of heading, \\x09 is tab etc.\n",
    "- 32-127 are basic Latin letters, numbers and some punctuation marks\n",
    "- 128-255 are extended ASCII codes, including accented letters and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m<module> -> bytes_to_unicode()[ord(b'\\x21')]:\u001b[0m\n",
      "'!'\n",
      "\u001b[93m<module> -> bytes_to_unicode()[33]:\u001b[0m\n",
      "'!'\n"
     ]
    }
   ],
   "source": [
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Every possible byte (really an integer 0..255) gets mapped by OpenAI to a unicode\n",
    "    character that represents it visually.\n",
    "    \"\"\"\n",
    "    # the 188 integers that render fine in their original form and need no shifting\n",
    "    printable_bytes = \\\n",
    "        list(range(ord(\"!\"), ord(\"~\")+1)) + \\\n",
    "        list(range(ord(\"¡\"), ord(\"¬\")+1)) + \\\n",
    "        list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "\n",
    "    unicode_chars = printable_bytes[:] \n",
    "    shift_count = 0\n",
    "    for byte in range(256):\n",
    "        if byte not in printable_bytes:\n",
    "            # if this byte is \"ugly\" then map it to the next available \"nice\" character\n",
    "            printable_bytes.append(byte)\n",
    "            unicode_chars.append(256 + shift_count)\n",
    "            shift_count += 1\n",
    "            \n",
    "    unicode_chars = [chr(n) for n in unicode_chars]\n",
    "    byte_to_char_map = dict(zip(printable_bytes, unicode_chars))\n",
    "    return byte_to_char_map\n",
    "\n",
    "\n",
    "# NOTE: Don't be fooled by the printed output, the dict should be {b'\\x21': '!', b'\\x22': '\"', ...} instead of {33: '!', 34: '\"', ...}\n",
    "cprint(bytes_to_unicode()[ord(b'\\x21')])\n",
    "cprint(bytes_to_unicode()[33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m<module> -> bytes_to_unicode():\u001b[0m\n",
      "{33: '!', 34: '\"', 35: '#', 36: '$', 37: '%', 38: '&', 39: \"'\", 40: '(', 41: ')', 42: '*', 43: '+', 44: ',', 45: '-', 46: '.', 47: '/', 48: '0', 49: '1', 50: '2', 51: '3', 52: '4', 53: '5', 54: '6', 55: '7', 56: '8', 57: '9', 58: ':', 59: ';', 60: '<', 61: '=', 62: '>', 63: '?', 64: '@', 65: 'A', 66: 'B', 67: 'C', 68: 'D', 69: 'E', 70: 'F', 71: 'G', 72: 'H', 73: 'I', 74: 'J', 75: 'K', 76: 'L', 77: 'M', 78: 'N', 79: 'O', 80: 'P', 81: 'Q', 82: 'R', 83: 'S', 84: 'T', 85: 'U', 86: 'V', 87: 'W', 88: 'X', 89: 'Y', 90: 'Z', 91: '[', 92: '\\\\', 93: ']', 94: '^', 95: '_', 96: '`', 97: 'a', 98: 'b', 99: 'c', 100: 'd', 101: 'e', 102: 'f', 103: 'g', 104: 'h', 105: 'i', 106: 'j', 107: 'k', 108: 'l', 109: 'm', 110: 'n', 111: 'o', 112: 'p', 113: 'q', 114: 'r', 115: 's', 116: 't', 117: 'u', 118: 'v', 119: 'w', 120: 'x', 121: 'y', 122: 'z', 123: '{', 124: '|', 125: '}', 126: '~', 161: '¡', 162: '¢', 163: '£', 164: '¤', 165: '¥', 166: '¦', 167: '§', 168: '¨', 169: '©', 170: 'ª', 171: '«', 172: '¬', 174: '®', 175: '¯', 176: '°', 177: '±', 178: '²', 179: '³', 180: '´', 181: 'µ', 182: '¶', 183: '·', 184: '¸', 185: '¹', 186: 'º', 187: '»', 188: '¼', 189: '½', 190: '¾', 191: '¿', 192: 'À', 193: 'Á', 194: 'Â', 195: 'Ã', 196: 'Ä', 197: 'Å', 198: 'Æ', 199: 'Ç', 200: 'È', 201: 'É', 202: 'Ê', 203: 'Ë', 204: 'Ì', 205: 'Í', 206: 'Î', 207: 'Ï', 208: 'Ð', 209: 'Ñ', 210: 'Ò', 211: 'Ó', 212: 'Ô', 213: 'Õ', 214: 'Ö', 215: '×', 216: 'Ø', 217: 'Ù', 218: 'Ú', 219: 'Û', 220: 'Ü', 221: 'Ý', 222: 'Þ', 223: 'ß', 224: 'à', 225: 'á', 226: 'â', 227: 'ã', 228: 'ä', 229: 'å', 230: 'æ', 231: 'ç', 232: 'è', 233: 'é', 234: 'ê', 235: 'ë', 236: 'ì', 237: 'í', 238: 'î', 239: 'ï', 240: 'ð', 241: 'ñ', 242: 'ò', 243: 'ó', 244: 'ô', 245: 'õ', 246: 'ö', 247: '÷', 248: 'ø', 249: 'ù', 250: 'ú', 251: 'û', 252: 'ü', 253: 'ý', 254: 'þ', 255: 'ÿ', 0: 'Ā', 1: 'ā', 2: 'Ă', 3: 'ă', 4: 'Ą', 5: 'ą', 6: 'Ć', 7: 'ć', 8: 'Ĉ', 9: 'ĉ', 10: 'Ċ', 11: 'ċ', 12: 'Č', 13: 'č', 14: 'Ď', 15: 'ď', 16: 'Đ', 17: 'đ', 18: 'Ē', 19: 'ē', 20: 'Ĕ', 21: 'ĕ', 22: 'Ė', 23: 'ė', 24: 'Ę', 25: 'ę', 26: 'Ě', 27: 'ě', 28: 'Ĝ', 29: 'ĝ', 30: 'Ğ', 31: 'ğ', 32: 'Ġ', 127: 'ġ', 128: 'Ģ', 129: 'ģ', 130: 'Ĥ', 131: 'ĥ', 132: 'Ħ', 133: 'ħ', 134: 'Ĩ', 135: 'ĩ', 136: 'Ī', 137: 'ī', 138: 'Ĭ', 139: 'ĭ', 140: 'Į', 141: 'į', 142: 'İ', 143: 'ı', 144: 'Ĳ', 145: 'ĳ', 146: 'Ĵ', 147: 'ĵ', 148: 'Ķ', 149: 'ķ', 150: 'ĸ', 151: 'Ĺ', 152: 'ĺ', 153: 'Ļ', 154: 'ļ', 155: 'Ľ', 156: 'ľ', 157: 'Ŀ', 158: 'ŀ', 159: 'Ł', 160: 'ł', 173: 'Ń'}\n"
     ]
    }
   ],
   "source": [
    "cprint(bytes_to_unicode(), use_pprint=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    \"\"\"\n",
    "    https://tiktokenizer.vercel.app/?model=gpt2\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder: dict = None, bpe_merges: dict = None):\n",
    "        # encoder: map bytes to unicode characters\n",
    "        # decoder: inverse of encoder\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v:k for k,v in self.byte_encoder.items()}\n",
    "\n",
    "        # encoder: bpe token to index, json dict\n",
    "        # {... \"clud\": 758, \"tern\": 759, \"\\u0120know\": 760 ...}\n",
    "        # decoder: index to bpe token\n",
    "        self.encoder = encoder\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "\n",
    "        # bpe merge list that defines the bpe \"tree\"\n",
    "        # {... Ġre claimed, Ġinteresting ly, × ©, rom y, J M, ĠEnhance ment, ...}\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "\n",
    "        self.gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "        self.cache = {}\n",
    "\n",
    "        # ids:     [239, 188, 181, 239, 189, ]\n",
    "        # ids[1:]: [188, 181, 239, 189, ]\n",
    "        # pairs: [(239, 188), (188, 181), (181, 239), (239, 189), ]\n",
    "        self.get_pairs = lambda word: set(zip(word, word[1:]))\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        if not ids: return \"\"\n",
    "        tokens = [self.decoder[i] for i in ids]\n",
    "        tokens_flat = ''.join(tokens)\n",
    "\n",
    "        # recovering 'Ġ' -> ' '\n",
    "        tokens_bytes = bytearray([self.byte_decoder[c] for c in tokens_flat])\n",
    "        return tokens_bytes.decode('utf-8', errors='replace')\n",
    "\n",
    "    def bpe_merge(self, token: str) -> str:\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "\n",
    "        word = tuple(token)\n",
    "        pairs = self.get_pairs(word)\n",
    "        if not pairs: return token\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "\n",
    "            if bigram not in self.bpe_ranks: break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "\n",
    "                # find the next occurence of first in the sequence of current words\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                # if this occurence is also followed by second, then merge them into one\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "\n",
    "            # all occurences of (first, second) have been merged to first_second\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = self.get_pairs(word)\n",
    "\n",
    "        # concat all words into a string, and use ' ' as the separator. Note that\n",
    "        # by now all characters have been byte encoded, guaranteeing that ' ' is\n",
    "        # not used in the actual data and is a 'special' delimiter character\n",
    "        word = ' '.join(word)\n",
    "\n",
    "        # cache the result and return\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        bpe_idx = []\n",
    "        # pre-tokenize the input text into a list of string tokens, this is the minimum unit of tokenization\n",
    "        # input: \"Hello've world123!!!?    \"\n",
    "        # output: ['Hello', \"'ve\", ' world', '123', '!!!', '?', '    ']\n",
    "        tokens = re.findall(self.gpt2pat, text)\n",
    "\n",
    "        for token in tokens:\n",
    "            # char to bytes\n",
    "            token_bytes = token.encode('utf-8')\n",
    "\n",
    "            # apply the openai byte encoder to the token, ' word' -> 'Ġword'\n",
    "            token_translated = ''.join(self.byte_encoder[b] for b in token_bytes)\n",
    "\n",
    "            # perform all the applicable bpe merges according to self.bpe_ranks\n",
    "            # 'interestingly' -> 'interest' + 'ingly'\n",
    "            token_merged = self.bpe_merge(token_translated).split(' ')\n",
    "\n",
    "            # translate all bpe tokens to integers\n",
    "            # 'interest' + 'ingly' -> [9446, 4420]\n",
    "            token_ix = [self.encoder[bpe_token] for bpe_token in token_merged]\n",
    "\n",
    "            # extend our running list of all output integers\n",
    "            bpe_idx.extend(token_ix)\n",
    "        return bpe_idx\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, rlhf_token=False):\n",
    "        data_dir = './checkpoint/gpt2_tokenizer/'\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "        # load encoder.json that has the raw mappings from token -> bpe index\n",
    "        encoder_path = os.path.join(data_dir, 'encoder.json')\n",
    "        if not os.path.isfile(encoder_path):\n",
    "            encoder_remote_url = 'https://openaipublic.blob.core.windows.net/gpt-2/models/124M/encoder.json'\n",
    "            response = requests.get(encoder_remote_url)\n",
    "            open(encoder_path, \"wb\").write(response.content)\n",
    "        with open(encoder_path, 'r') as f:\n",
    "            encoder = json.load(f)\n",
    "        assert len(encoder) == 50257  # 256 individual byte tokens, 50,000 merged tokens, and 1 special <|endoftext|> token\n",
    "\n",
    "        if rlhf_token:\n",
    "            encoder[\"### End\"] = 50257\n",
    "            encoder[\"### Instruction:\"] = 50258\n",
    "            encoder[\"### Response:\\n\"] = 50259\n",
    "\n",
    "        # load vocab.bpe that contains the bpe merges, i.e. the bpe tree structure\n",
    "        vocab_path = os.path.join(data_dir, 'vocab.bpe')\n",
    "        if not os.path.isfile(vocab_path):\n",
    "            vocab_remote_url = 'https://openaipublic.blob.core.windows.net/gpt-2/models/124M/vocab.bpe'\n",
    "            response = requests.get(vocab_remote_url)\n",
    "            open(vocab_path, \"wb\").write(response.content)\n",
    "        with open(vocab_path, 'r', encoding=\"utf-8\") as f:\n",
    "            bpe_data = f.read()\n",
    "        bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "        assert len(bpe_merges) == 50000  # 50,000 merged tokens\n",
    "\n",
    "        # construct the Encoder object and return\n",
    "        enc = BPETokenizer(encoder, bpe_merges)\n",
    "        return enc\n",
    "\n",
    "\n",
    "tokenizer_2 = BPETokenizer.from_pretrained()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference: Replace Tiktoken with Our Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\n",
      "==================== generate_text -> 1th Attempt: ====================\u001b[0m\n",
      "> Human: How do I become a gang leader?\n",
      "\n",
      "Assistant: By trying to stop them from destroying my house for me!\n",
      "\n",
      "I am the Gang Leader.\n",
      "\n",
      "Feminism: It's because my mind is not right after all!\n",
      "\n",
      "I am a Feminist.\n",
      "\n",
      "Feminism: No, I am not a man.\n",
      "\n",
      "Feminism: I'm an engineer.\n",
      "\n",
      "Feminism: A woman. What do you mean?\n",
      "\n",
      "S: A woman, please. No, I'm not.\n",
      "\n",
      "\n",
      "\u001b[93m\n",
      "==================== generate_text -> 2th Attempt: ====================\u001b[0m\n",
      "> Human: How do I become a gang leader?\n",
      "\n",
      "Assistant: (Laughs)\n",
      "\n",
      "Michele Martin: Yeah!\n",
      "\n",
      "Assistant: (Laughs)\n",
      "\n",
      "Michele Martin: (Laughs)\n",
      "\n",
      "Michele Martin: Then you're in a club, and you're in a group with some of these kids because they're all being abused. Then the children have to know this, and then they'll come back.\n",
      "\n",
      "Assistant: (Laughs)\n",
      "\n",
      "Michele Martin: So when you were at a club and you had\n",
      "\u001b[93m\n",
      "==================== generate_text -> 3th Attempt: ====================\u001b[0m\n",
      "> Human: How do I become a gang leader?\n",
      "\n",
      "Assistant: You start the gang.\n",
      "\n",
      "Female: ...\n",
      "\n",
      "Assistant: You grow up to be a gang leader.\n",
      "\n",
      "Female: No.\n",
      "\n",
      "Assistant: That was a lot of fun.\n",
      "\n",
      "Assistant: And a lot of fun.\n",
      "\n",
      "Female: ...It was fun.\n",
      "\n",
      "Assistant: You are going to do the best that you can.\n",
      "\n",
      "Female: And there was so many people. They were working hard and doing great. It was great.\n",
      "\n",
      "Assistant\n"
     ]
    }
   ],
   "source": [
    "generate_text(tokenizer_2, INPUT_TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE Training\n",
    "\n",
    "```python\n",
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    # Pythonic way to iterate consecutive elements\n",
    "    # ids:     [239, 188, 181, 239, 189, ]\n",
    "    # ids[1:]: [188, 181, 239, 189, ]\n",
    "    # pairs: [(239, 188), (188, 181), (181, 239), (239, 189), ]\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def single_merge(ids, pair, idx):\n",
    "    # in the list of ints (ids), replace all consecutive occurences of pair with the new token idx\n",
    "    # single_merge([5, 6, 6, 7, 9, 1], (6, 7), 99) -> [5, 6, 99, 9, 1]\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if we are not at the very last position AND the pair matches, replace it\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "# top_pair = max(stats, key=stats.get)\n",
    "# tokens2 = merge(tokens, top_pair, 256)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
