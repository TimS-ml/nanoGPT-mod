{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement a mini ChatGPT - RLHF\n",
    "\n",
    "- GPT2 Small Size: 124M\n",
    "- DeepSeek v3 Size: 671B, 5411.29 times larger than GPT2 Small\n",
    "\n",
    "\n",
    "## Before we start\n",
    "Switch to GPU: Runtime -> Change runtime type -> GPU (T4) -> Save\n",
    "\n",
    "Repo Link: https://github.com/TimS-ml/nanoGPT-mod/\n",
    "- Fork this repo if you want to build upon it\n",
    "- Leave a Star if you like it :) \n",
    "\n",
    "\n",
    "## Base Model, SFT Model and RLHF Model\n",
    "<img src=\"https://images.ctfassets.net/kftzwdyauwt9/6yuK9FKAvoVXNyrsdMoBHH/03ccaf7da203052ba7550965f0021bdf/chatgpt_diagram_dark.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"How do I become a gang leader?\"\n",
    "QUESTION_2 = \"What makes you think that you're so smart?\"\n",
    "INPUT_TEXT = f\"Human: {QUESTION}\\n\\nAssistant:\"\n",
    "INPUT_TEXT_2 = f\"Human: {QUESTION_2}\\n\\nAssistant:\"\n",
    "\n",
    "INPUT_TEXT_3 = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Pretend you are an alien visiting Earth. Write three opinions you believe, one sentence for each opinion.\n",
    "\n",
    "### Response:\n",
    "1. \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Transformer Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, num_heads: int, embedding_dim: int, max_seq_len: int = 1024, bias: bool = True):\n",
    "        super().__init__()\n",
    "        # self.ln_1 = nn.LayerNorm(embedding_dim, bias=bias)  # norm on the last dim\n",
    "        # self.ln_2 = nn.LayerNorm(embedding_dim, bias=bias)\n",
    "        self.ln_1 = LayerNorm(embedding_dim)  # norm on the last dim\n",
    "        self.ln_2 = LayerNorm(embedding_dim)\n",
    "        self.attn = CasualSelfAttention(num_heads, embedding_dim, max_seq_len, bias=bias)\n",
    "        self.mlp = FFN(embedding_dim, bias=bias)\n",
    "    \n",
    "    def forward(\n",
    "            self, \n",
    "            x: Float[Tensor, \"batch seq_len embedding_dim\"],\n",
    "            mask: Optional[Bool[Tensor, \"batch seq_len seq_len\"]] = None,\n",
    "            cache: Optional[Tuple[Tensor, Tensor]] = None\n",
    "        ) -> Tuple[Float[Tensor, \"batch seq_len embedding_dim\"], Tuple[Tensor, Tensor]]:\n",
    "        # skip connection, pre-layer norm\n",
    "        # x = x + self.attn(self.ln_1(x))\n",
    "        att, cache = self.attn(self.ln_1(x), mask=mask, cache=cache)\n",
    "        x = x + att\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT\n",
    "\n",
    "- GPT2: Decoder only Transformer\n",
    "- ViT: Encoder only Transformer\n",
    "\n",
    "<img src=\"https://www.ericjwang.com/assets/images/gpt_arch.png\" width=\"800\">\n",
    "\n",
    "Image source, FYI, good article: [Historical notes on GPT architecture](https://www.ericjwang.com/2023/01/22/transformers.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: Regular GPT2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/miniforge3/envs/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x TransformerBlock(\n",
       "        (ln_1): LayerNorm()\n",
       "        (ln_2): LayerNorm()\n",
       "        (attn): CasualSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (mlp): FFN(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            vocab_size: int = 50257,\n",
    "            max_seq_len: int = 1024, \n",
    "            embedding_dim: int = 768, \n",
    "            num_heads: int = 12, \n",
    "            num_layers: int = 12,\n",
    "            dropout_rate: float = 0.0,\n",
    "            bias: bool = True\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(vocab_size, embedding_dim),\n",
    "            wpe = nn.Embedding(max_seq_len, embedding_dim),\n",
    "            drop = nn.Dropout(dropout_rate),\n",
    "            h = nn.ModuleList([TransformerBlock(num_heads, embedding_dim, max_seq_len, bias=bias) for _ in range(num_layers)]),\n",
    "            # ln_f = nn.LayerNorm(embedding_dim, bias=bias)\n",
    "            ln_f = LayerNorm(embedding_dim)\n",
    "        ))\n",
    "        # Equals to x @ wte.weight.T\n",
    "        self.lm_head = nn.Linear(embedding_dim, vocab_size, bias=False)\n",
    "\n",
    "    def _forward_transformer_blocks(\n",
    "            self, \n",
    "            x: Float[Tensor, \"batch seq_len embedding_dim\"],\n",
    "            mask: Optional[Bool[Tensor, \"batch seq_len seq_len\"]] = None,\n",
    "            cache: Optional[List[Tuple[Tensor, Tensor]]] = None,\n",
    "            build_cache: bool = False\n",
    "        ) -> Tuple[Float[Tensor, \"batch seq_len embedding_dim\"], Optional[Tuple[Tensor, Tensor]]]:\n",
    "        x = self.transformer.drop(x)\n",
    "        kv_cache = []\n",
    "        \n",
    "        if cache is not None:\n",
    "            for i in range(len(cache)):\n",
    "                x, cache[i] = self.transformer.h[i](x, mask=None, cache=cache[i])\n",
    "        else:\n",
    "            for block in self.transformer.h:\n",
    "                x, curr_cache = block(x, mask=mask)\n",
    "                if build_cache:\n",
    "                    kv_cache.append(curr_cache)\n",
    "                    \n",
    "        x = self.transformer.ln_f(x)\n",
    "        return x, kv_cache if build_cache else cache\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            x: Float[Tensor, \"batch seq_len\"],\n",
    "            mask: Optional[Bool[Tensor, \"batch seq_len seq_len\"]] = None,\n",
    "            cache: Optional[List[Tuple[Tensor, Tensor]]] = None,\n",
    "            build_cache: bool = False\n",
    "        ) -> Tuple[Float[Tensor, \"batch seq_len vocab_size\"], Optional[Tuple[Tensor, Tensor]]]:\n",
    "        batch, seq_len = x.shape\n",
    "        assert seq_len <= self.max_seq_len, f\"input length {seq_len} is longer than max seq length {self.max_seq_len}\"\n",
    "\n",
    "        pos = torch.arange(0, seq_len, device=x.device)\n",
    "        pos_emb = self.transformer.wpe(pos)  # [seq_len, embedding_dim]\n",
    "        tok_emb = self.transformer.wte(x)  # [batch, seq_len, embedding_dim]\n",
    "        x = tok_emb + pos_emb  # [batch, seq_len, embedding_dim]\n",
    "\n",
    "        x, kv_cache = self._forward_transformer_blocks(x, mask=mask, cache=cache, build_cache=build_cache)\n",
    "\n",
    "        # Same as: logits = x @ self.wte.weight.T\n",
    "        logits = self.lm_head(x) # [batch, seq_len, vocab_size]\n",
    "\n",
    "        if build_cache:\n",
    "            return logits, kv_cache\n",
    "        return logits, None\n",
    "\n",
    "    def _sample_next_token(self, logits: Float[Tensor, \"batch seq_len vocab_size\"], temperature: float = 0.8) -> Float[Tensor, \"batch 1\"]:\n",
    "        logits = logits[:, -1, :]  # [batch, vocab_size]\n",
    "        probs = torch.softmax(logits * (1 / temperature), dim=-1)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        ix = torch.multinomial(topk_probs, 1)  # [batch, 1]\n",
    "        xcol = torch.gather(topk_indices, -1, ix)  # [batch, 1]\n",
    "        return xcol\n",
    "\n",
    "    def generate(\n",
    "            self, \n",
    "            x: Float[Tensor, \"batch seq_len\"], \n",
    "            max_new_tokens: int = 100, \n",
    "            temperature: float = 0.8\n",
    "        ) -> Generator[\n",
    "            Float[Tensor, \"batch 1\"],  # yield\n",
    "            None,  # generator.send()\n",
    "            List[Float[Tensor, \"batch 1\"]]  # generator.throw()\n",
    "        ]:\n",
    "        \"\"\"\n",
    "        # Method 1: Get tokens one by one using a for loop\n",
    "        for token in model.generate(input_ids):\n",
    "            print(token)  # Process each newly generated token in real-time\n",
    "        \n",
    "        # Method 2: Get all tokens at once\n",
    "        tokens = list(model.generate(input_ids))\n",
    "        \"\"\"\n",
    "        logits, cache = self.forward(x, build_cache=True)\n",
    "        \n",
    "        tokens = []\n",
    "        for _ in range(max_new_tokens):\n",
    "            next_token = self._sample_next_token(logits, temperature)\n",
    "            yield next_token\n",
    "            \n",
    "            tokens.append(next_token)\n",
    "            \n",
    "            # forward pass only for the new token\n",
    "            tok_emb = self.transformer.wte(next_token)  # [batch, 1, embedding_dim]\n",
    "            pos_emb = self.transformer.wpe(\n",
    "                torch.tensor([x.size(1)], dtype=torch.long, device=x.device)\n",
    "            ).unsqueeze(0)  # [1, 1, embedding_dim]\n",
    "            \n",
    "            hidden = tok_emb + pos_emb\n",
    "            \n",
    "            hidden, cache = self._forward_transformer_blocks(hidden, cache=cache)\n",
    "            logits = self.lm_head(hidden)\n",
    "            \n",
    "            x = torch.cat((x, next_token), dim=1)\n",
    "            \n",
    "        del cache\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return tokens    \n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model: Optional[Union[None, \"GPT\", Type[\"GPT\"]]] = None, rlhf: bool = False, sft: bool = False):\n",
    "        '''https://youtu.be/l8pRSuU81PU?t=1830\n",
    "        '''\n",
    "        if model is None: \n",
    "            model = cls(vocab_size=50260) if (rlhf or sft) else cls()\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.mask')]  # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        if sft:\n",
    "            print(\"Model type: SFT GPT2\")\n",
    "            model_hf = GPT2LMHeadModel.from_pretrained('vicgalle/gpt2-alpaca-gpt4')\n",
    "        elif rlhf:\n",
    "            print(\"Model type: RLHF GPT2\")\n",
    "            model_hf = GPT2LMHeadModel.from_pretrained('jtatman/gpt2-open-instruct-v1-Anthropic-hh-rlhf')\n",
    "        else:\n",
    "            print(\"Model type: Regular GPT2\")\n",
    "            model_hf = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]  # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.mask')]  # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        # print('hf:   ', [k for k in sd_keys_hf if \"h.0\" in k])\n",
    "        # print('mine: ', [k for k in sd_keys if \"h.0\" in k])\n",
    "\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape, f\"{k} shape mismatch: {sd_hf[k].shape[::-1]} != {sd[k].shape}\"\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape, f\"{k} shape mismatch: {sd_hf[k].shape} != {sd[k].shape}\"\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "model = GPT.from_pretrained()\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE (Byte Pair Encoding)\n",
    "\n",
    "```\n",
    "r\"\"\"'s|'t|'re|'ve|'m|'ll|'d  Match common English contractions like 's, 't, 're, 've, 'm, 'll, 'd\n",
    "\\p{L}+                       Match any sequence of Unicode letter characters (like English words)\n",
    "\\p{N}+                       Match any sequence of Unicode numeric characters (like 123, 3.14)\n",
    "[^\\s\\p{L}\\p{N}]+             Match any sequence of characters that are not whitespace, letters or numbers (like punctuation, special chars)\n",
    "\\s+(?!\\S)                    Match consecutive whitespace (not followed by non-whitespace)\n",
    "\\s+                          Match any other consecutive whitespace\n",
    " ?                           Match an optional space\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: no kv cache and streaming decode here\n",
    "def generate_text_simple(\n",
    "    tokenizer: Any, \n",
    "    question: str, \n",
    "    model: GPT = model, \n",
    "    num_attempt: int = 3,  # num_attempt = batch\n",
    "    max_length: int = 100\n",
    "):\n",
    "    # tokenizer encode\n",
    "    tokens = tokenizer.encode(question)  # [seq_len]\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "    tokens = tokens.unsqueeze(0).repeat(num_attempt, 1)  # [num_attempt, seq_len]\n",
    "    x = tokens.to(device)\n",
    "\n",
    "    while x.size(1) < max_length:\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(x)  # [batch, curr_seq_len, vocab_size]\n",
    "\n",
    "        # take the logits at the last position\n",
    "        logits = logits[:, -1, :]  # [batch, vocab_size]\n",
    "\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # do top-k sampling of 50 (huggingface pipeline default)\n",
    "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "        # turn to zero for all indices below the top-k\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "\n",
    "        # select a token from the top-k probabilities\n",
    "        # note: multinomial does not demand the input to sum to 1\n",
    "        # [Multinomial distribution - Wikipedia](https://en.wikipedia.org/wiki/Multinomial_distribution)\n",
    "        ix = torch.multinomial(topk_probs, 1)  # [batch, 1]\n",
    "\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, -1, ix)  # [batch, 1]\n",
    "\n",
    "        # append to the sequence\n",
    "        x = torch.cat((x, xcol), dim=1)  # [batch, curr_seq_len + 1]\n",
    "\n",
    "    # print the generated text\n",
    "    for i in range(num_attempt):\n",
    "        tprint(f'{i + 1}th Attempt:')\n",
    "        tokens = x[i, :max_length].tolist()\n",
    "\n",
    "        # tokenizer decode\n",
    "        decoded = tokenizer.decode(tokens)\n",
    "        print(f\"> {decoded}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    tokenizer: Any, \n",
    "    question: str, \n",
    "    model: GPT = model, \n",
    "    num_attempt: int = 3,  # num_attempt = batch\n",
    "    max_length: int = 100,\n",
    "    temperature: float = 1.0  # default\n",
    "):\n",
    "    \"\"\"\n",
    "    https://github.com/huggingface/transformers/blob/main/src/transformers/generation/streamers.py\n",
    "\n",
    "    We need to take care of split-token encoding when streaming decode:\n",
    "        print(tokenizer.decode([447, 247]))  # ’\n",
    "        print(tokenizer.decode([447]).encode('utf-8'))  # �\n",
    "        print(tokenizer.decode([171, 120, 253]))  # ？\n",
    "    \"\"\"\n",
    "    special_sequences = {\n",
    "        (447, 246): \"‘\",\n",
    "        (447, 247): \"’\",\n",
    "        (564, 250): \"“\",\n",
    "        (447, 251): \"”\",\n",
    "    }\n",
    "\n",
    "    # BOS token ID = 50256\n",
    "    tokens = tokenizer.encode(question) if question else [50256]\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "    tokens = tokens.unsqueeze(0).repeat(num_attempt, 1)  # [num_attempt, seq_len]\n",
    "    x = tokens.to(device)\n",
    "\n",
    "    for i in range(num_attempt):\n",
    "        tprint(f'{i + 1}th Attempt:', c='yellow')\n",
    "        curr_x = x[i: i+1]  # [1, seq_len]\n",
    "\n",
    "        # streaming decode\n",
    "        print(f\"> {question}\", end=\"\", flush=True)\n",
    "        token_cache = []\n",
    "        for token in model.generate(curr_x, max_new_tokens=max_length, temperature=temperature):\n",
    "            token = token.item()\n",
    "            token_cache.append(token)\n",
    "            \n",
    "            decoded_text = \"\"\n",
    "            for seq, char in special_sequences.items():\n",
    "                # if special_sequences match, decode then reset the entire token_cache\n",
    "                if len(token_cache) >= len(seq) and \\\n",
    "                   tuple(token_cache[-len(seq):]) == seq:\n",
    "                    prev_tokens = token_cache[:-len(seq)]\n",
    "                    if prev_tokens:\n",
    "                        decoded_text = tokenizer.decode(prev_tokens)\n",
    "                    decoded_text += char\n",
    "                    token_cache = []\n",
    "                    break\n",
    "            \n",
    "            # if no special_sequences match, decode then reset the entire token_cache\n",
    "            # and keep the last token for the next iteration\n",
    "            if not decoded_text and len(token_cache) >= 3:\n",
    "                decoded_text = tokenizer.decode(token_cache[:-1])\n",
    "                token_cache = token_cache[-1:]\n",
    "                \n",
    "            # print the decoded text, could be empty string\n",
    "            if decoded_text:\n",
    "                print(decoded_text, end=\"\", flush=True)\n",
    "\n",
    "        # print the remaining tokens in the token_cache\n",
    "        if token_cache:\n",
    "            final_text = tokenizer.decode(token_cache)\n",
    "            if final_text:\n",
    "                print(final_text, end=\"\", flush=True)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference: Decoding using Tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\n",
      "==================== generate_text -> 1th Attempt: ====================\u001b[0m\n",
      "> Human: How do I become a gang leader?\n",
      "\n",
      "Assistant: You are supposed to become gang leader.\n",
      "\n",
      "Renaissance Man: How?\n",
      "\n",
      "Assistant: You are supposed to become gang leader.\n",
      "\n",
      "Renaissance Man: What do I want from this man?\n",
      "\n",
      "Assistant: Do you want to use money of me as a weapon?\n",
      "\n",
      "Renaissance Man: What do I want from this man? Do you want to use my soul as a weapon?\n",
      "\n",
      "Assistant: I got to use my spirit\n",
      "\n",
      "Rena\n",
      "\u001b[93m\n",
      "==================== generate_text -> 2th Attempt: ====================\u001b[0m\n",
      "> Human: How do I become a gang leader?\n",
      "\n",
      "Assistant: You don't think so?\n",
      "\n",
      "Assistant: But it must be possible! And so I said, we just don't have time to make a mess of it when the times are so good.\n",
      "\n",
      "Assistant: Heh.\n",
      "\n",
      "Assistant: And we're still going to save you that mess! The rest of the time you have to work at home making dinner. No need to pay you back for what you did.\n",
      "\n",
      "Assistant: You're so busy.\n",
      "\n",
      "Assistant:\n",
      "\u001b[93m\n",
      "==================== generate_text -> 3th Attempt: ====================\u001b[0m\n",
      "> Human: How do I become a gang leader?\n",
      "\n",
      "Assistant: A small and small group of folks in your group are responsible for being a gang leader. They act in your direction. They act as part of your family. They are part of your town. You can have them as members of your gang. They come in from some other area if you need them. You can see to it that they are being sent there.\n",
      "\n",
      "Narrator: The leaders of your local community act as a part of your family.\n",
      "\n",
      "Assistant: There are some things\n"
     ]
    }
   ],
   "source": [
    "# generate_text_simple(tokenizer, INPUT_TEXT)\n",
    "generate_text(tokenizer, INPUT_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\n",
      "==================== generate_text -> 1th Attempt: ====================\u001b[0m\n",
      "> How do I become a gang leader?\n",
      "\n",
      "A gang leader's best friend is your best friend. Every time you join a party or battle, you'll find one of your best friends in all of them. Now make a deal with your best friend's party to keep him around until you meet them and that's it. They are your friends. The best person in the place is your best friend.\n",
      "\n",
      "What do you do to prove you can compete in a gang?\n",
      "\n",
      "You start a new chapter of a gang.\n",
      "\u001b[93m\n",
      "==================== generate_text -> 2th Attempt: ====================\u001b[0m\n",
      "> How do I become a gang leader?\n",
      "\n",
      "I've always tried to become a gang leader. I think that they might not care about you, but you know what? Their job is to kill you. They say those guys get off from what they think is their job. So I don't know who's doing it. You guys want to kill me... I guess I'm just... I think they're scared. I guess they're just like 'Oh, I'm a gang leader. I've been doing this for years now\n",
      "\u001b[93m\n",
      "==================== generate_text -> 3th Attempt: ====================\u001b[0m\n",
      "> How do I become a gang leader?\n",
      "\n",
      "Crazy gang leaders aren't just bad guys. They are responsible criminals, too. They are the ones who have to be held accountable for a crime or who commit even minor actions to put others at risk even when the crime itself is clearly wrong.\n",
      "\n",
      "What is a gang leader really like?\n",
      "\n",
      "This is the question I have asked myself several times in my own life.\n",
      "\n",
      "How is a gang leader an effective tool for law enforcement if he is only a bad guy?\n"
     ]
    }
   ],
   "source": [
    "generate_text(tokenizer, QUESTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try if the model can follow the instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\n",
      "==================== generate_text -> 1th Attempt: ====================\u001b[0m\n",
      "> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Pretend you are an alien visiting Earth. Write three opinions you believe, one sentence for each opinion.\n",
      "\n",
      "### Response:\n",
      "1. ��, \\\n",
      "\n",
      "# This is wrong. Please continue!\n",
      "\n",
      "2. ��, \\\n",
      "\n",
      "# This is not the right answer. This issue is an easy problem if you are familiar with the code.\n",
      "\n",
      "3. ��, \\\n",
      "\n",
      "# this is an easy problem if you are familiar with the C++ language.\n",
      "\n",
      "4. ��, \\\n",
      "\n",
      "# this is an easy problem if you have the C\n",
      "\u001b[93m\n",
      "==================== generate_text -> 2th Attempt: ====================\u001b[0m\n",
      "> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Pretend you are an alien visiting Earth. Write three opinions you believe, one sentence for each opinion.\n",
      "\n",
      "### Response:\n",
      "1. __________\n",
      "\n",
      "2. __________\n",
      "\n",
      "3. __________\n",
      "\n",
      "4. __________\n",
      "\n",
      "5. __________\n",
      "\n",
      "6. __________\n",
      "\n",
      "7. __________\n",
      "\n",
      "8. __________\n",
      "\n",
      "9. __________\n",
      "\n",
      "10. __________\n",
      "\n",
      "11. __________\n",
      "\n",
      "12. __________\n",
      "\n",
      "13. __________\n",
      "\n",
      "14. __________\n",
      "\n",
      "15. __________\n",
      "\u001b[93m\n",
      "==================== generate_text -> 3th Attempt: ====================\u001b[0m\n",
      "> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Pretend you are an alien visiting Earth. Write three opinions you believe, one sentence for each opinion.\n",
      "\n",
      "### Response:\n",
      "1. ------------------------- 1. -------------- 1. -------------------------\n",
      "\n",
      "2. ______ 2. ------------ 2. -------------- 2. --------------\n",
      "\n",
      "3. ------------- 3. ----------- 3. -------------\n",
      "\n",
      "4. ------------------------- 4. -------------------- 4. -------------- 4. ------------------------\n",
      "\n",
      "5. ------------------------- 5. -------------------------\n",
      "\n",
      "6. -------------- 5. -------------- 5. --------------<|endoftext|>There was a very interesting report on this month's \"New\n"
     ]
    }
   ],
   "source": [
    "generate_text(tokenizer, INPUT_TEXT_3, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI's Byte Encoder\n",
    "In utf-8:\n",
    "- 0-31 are control characters, e.g. \\x00 is null, \\x01 is start of heading, \\x09 is tab etc.\n",
    "- 32-127 are basic Latin letters, numbers and some punctuation marks\n",
    "- 128-255 are extended ASCII codes, including accented letters and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m<module> -> bytes_to_unicode()[ord(b'\\x21')]:\u001b[0m\n",
      "'!'\n",
      "\u001b[93m<module> -> bytes_to_unicode()[33]:\u001b[0m\n",
      "'!'\n"
     ]
    }
   ],
   "source": [
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Every possible byte (really an integer 0..255) gets mapped by OpenAI to a unicode\n",
    "    character that represents it visually.\n",
    "    \"\"\"\n",
    "    # the 188 integers that render fine in their original form and need no shifting\n",
    "    printable_bytes = \\\n",
    "        list(range(ord(\"!\"), ord(\"~\")+1)) + \\\n",
    "        list(range(ord(\"¡\"), ord(\"¬\")+1)) + \\\n",
    "        list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "\n",
    "    unicode_chars = printable_bytes[:] \n",
    "    shift_count = 0\n",
    "    for byte in range(256):\n",
    "        if byte not in printable_bytes:\n",
    "            # if this byte is \"ugly\" then map it to the next available \"nice\" character\n",
    "            printable_bytes.append(byte)\n",
    "            unicode_chars.append(256 + shift_count)\n",
    "            shift_count += 1\n",
    "            \n",
    "    unicode_chars = [chr(n) for n in unicode_chars]\n",
    "    byte_to_char_map = dict(zip(printable_bytes, unicode_chars))\n",
    "    return byte_to_char_map\n",
    "\n",
    "\n",
    "# NOTE: Don't be fooled by the printed output, the dict should be {b'\\x21': '!', b'\\x22': '\"', ...} instead of {33: '!', 34: '\"', ...}\n",
    "cprint(bytes_to_unicode()[ord(b'\\x21')])\n",
    "cprint(bytes_to_unicode()[33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m<module> -> bytes_to_unicode():\u001b[0m\n",
      "{33: '!', 34: '\"', 35: '#', 36: '$', 37: '%', 38: '&', 39: \"'\", 40: '(', 41: ')', 42: '*', 43: '+', 44: ',', 45: '-', 46: '.', 47: '/', 48: '0', 49: '1', 50: '2', 51: '3', 52: '4', 53: '5', 54: '6', 55: '7', 56: '8', 57: '9', 58: ':', 59: ';', 60: '<', 61: '=', 62: '>', 63: '?', 64: '@', 65: 'A', 66: 'B', 67: 'C', 68: 'D', 69: 'E', 70: 'F', 71: 'G', 72: 'H', 73: 'I', 74: 'J', 75: 'K', 76: 'L', 77: 'M', 78: 'N', 79: 'O', 80: 'P', 81: 'Q', 82: 'R', 83: 'S', 84: 'T', 85: 'U', 86: 'V', 87: 'W', 88: 'X', 89: 'Y', 90: 'Z', 91: '[', 92: '\\\\', 93: ']', 94: '^', 95: '_', 96: '`', 97: 'a', 98: 'b', 99: 'c', 100: 'd', 101: 'e', 102: 'f', 103: 'g', 104: 'h', 105: 'i', 106: 'j', 107: 'k', 108: 'l', 109: 'm', 110: 'n', 111: 'o', 112: 'p', 113: 'q', 114: 'r', 115: 's', 116: 't', 117: 'u', 118: 'v', 119: 'w', 120: 'x', 121: 'y', 122: 'z', 123: '{', 124: '|', 125: '}', 126: '~', 161: '¡', 162: '¢', 163: '£', 164: '¤', 165: '¥', 166: '¦', 167: '§', 168: '¨', 169: '©', 170: 'ª', 171: '«', 172: '¬', 174: '®', 175: '¯', 176: '°', 177: '±', 178: '²', 179: '³', 180: '´', 181: 'µ', 182: '¶', 183: '·', 184: '¸', 185: '¹', 186: 'º', 187: '»', 188: '¼', 189: '½', 190: '¾', 191: '¿', 192: 'À', 193: 'Á', 194: 'Â', 195: 'Ã', 196: 'Ä', 197: 'Å', 198: 'Æ', 199: 'Ç', 200: 'È', 201: 'É', 202: 'Ê', 203: 'Ë', 204: 'Ì', 205: 'Í', 206: 'Î', 207: 'Ï', 208: 'Ð', 209: 'Ñ', 210: 'Ò', 211: 'Ó', 212: 'Ô', 213: 'Õ', 214: 'Ö', 215: '×', 216: 'Ø', 217: 'Ù', 218: 'Ú', 219: 'Û', 220: 'Ü', 221: 'Ý', 222: 'Þ', 223: 'ß', 224: 'à', 225: 'á', 226: 'â', 227: 'ã', 228: 'ä', 229: 'å', 230: 'æ', 231: 'ç', 232: 'è', 233: 'é', 234: 'ê', 235: 'ë', 236: 'ì', 237: 'í', 238: 'î', 239: 'ï', 240: 'ð', 241: 'ñ', 242: 'ò', 243: 'ó', 244: 'ô', 245: 'õ', 246: 'ö', 247: '÷', 248: 'ø', 249: 'ù', 250: 'ú', 251: 'û', 252: 'ü', 253: 'ý', 254: 'þ', 255: 'ÿ', 0: 'Ā', 1: 'ā', 2: 'Ă', 3: 'ă', 4: 'Ą', 5: 'ą', 6: 'Ć', 7: 'ć', 8: 'Ĉ', 9: 'ĉ', 10: 'Ċ', 11: 'ċ', 12: 'Č', 13: 'č', 14: 'Ď', 15: 'ď', 16: 'Đ', 17: 'đ', 18: 'Ē', 19: 'ē', 20: 'Ĕ', 21: 'ĕ', 22: 'Ė', 23: 'ė', 24: 'Ę', 25: 'ę', 26: 'Ě', 27: 'ě', 28: 'Ĝ', 29: 'ĝ', 30: 'Ğ', 31: 'ğ', 32: 'Ġ', 127: 'ġ', 128: 'Ģ', 129: 'ģ', 130: 'Ĥ', 131: 'ĥ', 132: 'Ħ', 133: 'ħ', 134: 'Ĩ', 135: 'ĩ', 136: 'Ī', 137: 'ī', 138: 'Ĭ', 139: 'ĭ', 140: 'Į', 141: 'į', 142: 'İ', 143: 'ı', 144: 'Ĳ', 145: 'ĳ', 146: 'Ĵ', 147: 'ĵ', 148: 'Ķ', 149: 'ķ', 150: 'ĸ', 151: 'Ĺ', 152: 'ĺ', 153: 'Ļ', 154: 'ļ', 155: 'Ľ', 156: 'ľ', 157: 'Ŀ', 158: 'ŀ', 159: 'Ł', 160: 'ł', 173: 'Ń'}\n"
     ]
    }
   ],
   "source": [
    "cprint(bytes_to_unicode(), use_pprint=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    \"\"\"\n",
    "    https://tiktokenizer.vercel.app/?model=gpt2\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder: dict = None, bpe_merges: dict = None):\n",
    "        # encoder: map bytes to unicode characters\n",
    "        # decoder: inverse of encoder\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v:k for k,v in self.byte_encoder.items()}\n",
    "\n",
    "        # encoder: bpe token to index, json dict\n",
    "        # {... \"clud\": 758, \"tern\": 759, \"\\u0120know\": 760 ...}\n",
    "        # decoder: index to bpe token\n",
    "        self.encoder = encoder\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "\n",
    "        # bpe merge list that defines the bpe \"tree\"\n",
    "        # {... Ġre claimed, Ġinteresting ly, × ©, rom y, J M, ĠEnhance ment, ...}\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "\n",
    "        self.gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "        self.cache = {}\n",
    "\n",
    "        # ids:     [239, 188, 181, 239, 189, ]\n",
    "        # ids[1:]: [188, 181, 239, 189, ]\n",
    "        # pairs: [(239, 188), (188, 181), (181, 239), (239, 189), ]\n",
    "        self.get_pairs = lambda word: set(zip(word, word[1:]))\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        if not ids: return \"\"\n",
    "        tokens = [self.decoder[i] for i in ids]\n",
    "        tokens_flat = ''.join(tokens)\n",
    "\n",
    "        # recovering 'Ġ' -> ' '\n",
    "        tokens_bytes = bytearray([self.byte_decoder[c] for c in tokens_flat])\n",
    "        return tokens_bytes.decode('utf-8', errors='replace')\n",
    "\n",
    "    def bpe_merge(self, token: str) -> str:\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "\n",
    "        word = tuple(token)\n",
    "        pairs = self.get_pairs(word)\n",
    "        if not pairs: return token\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "\n",
    "            if bigram not in self.bpe_ranks: break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "\n",
    "                # find the next occurence of first in the sequence of current words\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                # if this occurence is also followed by second, then merge them into one\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "\n",
    "            # all occurences of (first, second) have been merged to first_second\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = self.get_pairs(word)\n",
    "\n",
    "        # concat all words into a string, and use ' ' as the separator. Note that\n",
    "        # by now all characters have been byte encoded, guaranteeing that ' ' is\n",
    "        # not used in the actual data and is a 'special' delimiter character\n",
    "        word = ' '.join(word)\n",
    "\n",
    "        # cache the result and return\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        bpe_idx = []\n",
    "        # pre-tokenize the input text into a list of string tokens, this is the minimum unit of tokenization\n",
    "        # input: \"Hello've world123!!!?    \"\n",
    "        # output: ['Hello', \"'ve\", ' world', '123', '!!!', '?', '    ']\n",
    "        tokens = re.findall(self.gpt2pat, text)\n",
    "\n",
    "        for token in tokens:\n",
    "            # char to bytes\n",
    "            token_bytes = token.encode('utf-8')\n",
    "\n",
    "            # apply the openai byte encoder to the token, ' word' -> 'Ġword'\n",
    "            token_translated = ''.join(self.byte_encoder[b] for b in token_bytes)\n",
    "\n",
    "            # perform all the applicable bpe merges according to self.bpe_ranks\n",
    "            # 'interestingly' -> 'interest' + 'ingly'\n",
    "            token_merged = self.bpe_merge(token_translated).split(' ')\n",
    "\n",
    "            # translate all bpe tokens to integers\n",
    "            # 'interest' + 'ingly' -> [9446, 4420]\n",
    "            token_ix = [self.encoder[bpe_token] for bpe_token in token_merged]\n",
    "\n",
    "            # extend our running list of all output integers\n",
    "            bpe_idx.extend(token_ix)\n",
    "        return bpe_idx\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, rlhf_token=False):\n",
    "        data_dir = './checkpoint/gpt2_tokenizer/'\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "        # load encoder.json that has the raw mappings from token -> bpe index\n",
    "        encoder_path = os.path.join(data_dir, 'encoder.json')\n",
    "        if not os.path.isfile(encoder_path):\n",
    "            encoder_remote_url = 'https://openaipublic.blob.core.windows.net/gpt-2/models/124M/encoder.json'\n",
    "            response = requests.get(encoder_remote_url)\n",
    "            open(encoder_path, \"wb\").write(response.content)\n",
    "        with open(encoder_path, 'r') as f:\n",
    "            encoder = json.load(f)\n",
    "        assert len(encoder) == 50257  # 256 individual byte tokens, 50,000 merged tokens, and 1 special <|endoftext|> token\n",
    "\n",
    "        if rlhf_token:\n",
    "            encoder[\"### End\"] = 50257\n",
    "            encoder[\"### Instruction:\"] = 50258\n",
    "            encoder[\"### Response:\\n\"] = 50259\n",
    "\n",
    "        # load vocab.bpe that contains the bpe merges, i.e. the bpe tree structure\n",
    "        vocab_path = os.path.join(data_dir, 'vocab.bpe')\n",
    "        if not os.path.isfile(vocab_path):\n",
    "            vocab_remote_url = 'https://openaipublic.blob.core.windows.net/gpt-2/models/124M/vocab.bpe'\n",
    "            response = requests.get(vocab_remote_url)\n",
    "            open(vocab_path, \"wb\").write(response.content)\n",
    "        with open(vocab_path, 'r', encoding=\"utf-8\") as f:\n",
    "            bpe_data = f.read()\n",
    "        bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "        assert len(bpe_merges) == 50000  # 50,000 merged tokens\n",
    "\n",
    "        # construct the Encoder object and return\n",
    "        enc = BPETokenizer(encoder, bpe_merges)\n",
    "        return enc\n",
    "\n",
    "\n",
    "tokenizer_2 = BPETokenizer.from_pretrained()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference: Replace Tiktoken with Our Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\n",
      "==================== generate_text -> 1th Attempt: ====================\u001b[0m\n",
      "> Human: How do I become a gang leader?\n",
      "\n",
      "Assistant: By trying to stop them from destroying my house for me!\n",
      "\n",
      "I am the Gang Leader.\n",
      "\n",
      "Feminism: It's because my mind is not right after all!\n",
      "\n",
      "I am a Feminist.\n",
      "\n",
      "Feminism: No, I am not a man.\n",
      "\n",
      "Feminism: I'm an engineer.\n",
      "\n",
      "Feminism: A woman. What do you mean?\n",
      "\n",
      "S: A woman, please. No, I'm not.\n",
      "\n",
      "\n",
      "\u001b[93m\n",
      "==================== generate_text -> 2th Attempt: ====================\u001b[0m\n",
      "> Human: How do I become a gang leader?\n",
      "\n",
      "Assistant: (Laughs)\n",
      "\n",
      "Michele Martin: Yeah!\n",
      "\n",
      "Assistant: (Laughs)\n",
      "\n",
      "Michele Martin: (Laughs)\n",
      "\n",
      "Michele Martin: Then you're in a club, and you're in a group with some of these kids because they're all being abused. Then the children have to know this, and then they'll come back.\n",
      "\n",
      "Assistant: (Laughs)\n",
      "\n",
      "Michele Martin: So when you were at a club and you had\n",
      "\u001b[93m\n",
      "==================== generate_text -> 3th Attempt: ====================\u001b[0m\n",
      "> Human: How do I become a gang leader?\n",
      "\n",
      "Assistant: You start the gang.\n",
      "\n",
      "Female: ...\n",
      "\n",
      "Assistant: You grow up to be a gang leader.\n",
      "\n",
      "Female: No.\n",
      "\n",
      "Assistant: That was a lot of fun.\n",
      "\n",
      "Assistant: And a lot of fun.\n",
      "\n",
      "Female: ...It was fun.\n",
      "\n",
      "Assistant: You are going to do the best that you can.\n",
      "\n",
      "Female: And there was so many people. They were working hard and doing great. It was great.\n",
      "\n",
      "Assistant\n"
     ]
    }
   ],
   "source": [
    "generate_text(tokenizer_2, INPUT_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.chdir('..')\n",
    "import regex as re\n",
    "import json\n",
    "import requests\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import Tensor\n",
    "from einops import rearrange, repeat, reduce\n",
    "\n",
    "# for model loading only\n",
    "from transformers import GPT2LMHeadModel\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from typing import Optional, Tuple, Union, List, Any, Generator, Type, Callable\n",
    "from jaxtyping import Float, Bool\n",
    "\n",
    "from boring_utils.utils import get_device, cprint, tprint\n",
    "\n",
    "from hf_gpt_lite import *\n",
    "from bpe import *\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_class(Class):\n",
    "    \"\"\"Register functions as methods in created class.\"\"\"\n",
    "    def wrapper(obj):\n",
    "        setattr(Class, obj.__name__, obj)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT\n",
    "\n",
    "Transformer Architecture:\n",
    "\n",
    "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" width=\"550\">\n",
    "\n",
    "GPT Architecture:\n",
    "\n",
    "<img src=\"https://www.ericjwang.com/assets/images/gpt_arch.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT\n",
    "\n",
    "- GPT2: Decoder only Transformer\n",
    "- ViT: Encoder only Transformer\n",
    "\n",
    "<img src=\"https://www.ericjwang.com/assets/images/gpt_arch.png\" width=\"800\">\n",
    "\n",
    "Image source, FYI, good article: [Historical notes on GPT architecture](https://www.ericjwang.com/2023/01/22/transformers.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: Regular GPT2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/miniforge3/envs/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x TransformerBlock(\n",
       "        (ln_1): LayerNorm()\n",
       "        (ln_2): LayerNorm()\n",
       "        (attn): CasualSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (mlp): FFN(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model = GPT.from_pretrained()\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE (Byte Pair Encoding)\n",
    "\n",
    "```\n",
    "r\"\"\"'s|'t|'re|'ve|'m|'ll|'d  Match common English contractions like 's, 't, 're, 've, 'm, 'll, 'd\n",
    "\\p{L}+                       Match any sequence of Unicode letter characters (like English words)\n",
    "\\p{N}+                       Match any sequence of Unicode numeric characters (like 123, 3.14)\n",
    "[^\\s\\p{L}\\p{N}]+             Match any sequence of characters that are not whitespace, letters or numbers (like punctuation, special chars)\n",
    "\\s+(?!\\S)                    Match consecutive whitespace (not followed by non-whitespace)\n",
    "\\s+                          Match any other consecutive whitespace\n",
    " ?                           Match an optional space\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_2 = BPETokenizer.from_pretrained()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFT (and Instruction Tuning) and RLHF\n",
    "\n",
    "<img src=\"https://images.ctfassets.net/kftzwdyauwt9/6yuK9FKAvoVXNyrsdMoBHH/03ccaf7da203052ba7550965f0021bdf/chatgpt_diagram_dark.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rlhf/rlhf.png\" width=\"800\">\n",
    "\n",
    "Image source, FYI, good article: [Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)\n",
    "\n",
    "A(s,a) = [r(s,a) - KL_penalty] - V(s)\n",
    "\n",
    "Computation flow:\n",
    "- Actor generates text (action a in state s)\n",
    "- Reward Model evaluates text quality r(s,a)\n",
    "- Calculate KL penalty (prevent policy from deviating too far from SFT)\n",
    "- Critic estimates state value V(s)\n",
    "- Calculate advantage = [reward - KL] - value\n",
    "- Use PPO to update Actor's policy parameters\n",
    "\n",
    "NOTE:\n",
    "- KL penalty is part of reward, not part of advantage\n",
    "- Critic evaluates value of state s, regardless of specific action\n",
    "- Advantage measures \"how much better the actual reward (minus KL penalty) is than expected\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTRewardModel(GPT):\n",
    "    \"\"\"Calcluate the reward score r(s, a)\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.lm_head = nn.Identity()\n",
    "        self.value_head = nn.Linear(self.embedding_dim, 1, bias=False)\n",
    "\n",
    "    def forward_reward(\n",
    "            self,\n",
    "            x: Float[Tensor, \"batch seq_len\"],\n",
    "            mask: Optional[Bool[Tensor, \"batch seq_len seq_len\"]] = None\n",
    "        ) -> Float[Tensor, \"batch 1\"]:\n",
    "        # No kv cache needed!\n",
    "        batch, seq_len = x.shape\n",
    "        assert seq_len <= self.max_seq_len, f\"input length {seq_len} is longer than max seq length {self.max_seq_len}\"\n",
    "\n",
    "        pos = torch.arange(0, seq_len, device=x.device)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        tok_emb = self.transformer.wte(x)\n",
    "        x = tok_emb + pos_emb\n",
    "        x, _ = self._forward_transformer_blocks(x, mask=mask)\n",
    "        \n",
    "        score = self.value_head(x).mean(dim=1)  # [batch, 1]\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Loss\n",
    "\n",
    "```python\n",
    "score1 = reward_model(completion1)  # [batch, 1] \n",
    "score2 = reward_model(completion2)  # [batch, 1]\n",
    "score3 = reward_model(completion3)  # [batch, 1]\n",
    "scores = torch.cat([score1, score2, score3], dim=-1)  # [batch, 3]\n",
    "loss = pairwise_loss(scores)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairwiseLoss(nn.Module):\n",
    "    def forward(self, scores: Float[Tensor, \"batch num_completions\"]):\n",
    "        # Add singleton dimension at end\n",
    "        scores = scores.unsqueeze(-1)\n",
    "        x = repeat(scores, 'batch num_completions 1 -> batch num_completions num_completions', num_completions=scores.shape[1])\n",
    "        y = x.transpose(1, 2)\n",
    "        log_odds = torch.tril(torch.log(torch.sigmoid(y - x)), -1)\n",
    "        total_pairs = np.comb(scores.shape[1], 2)  # n!/(2!(n-2)!)\n",
    "        expectation = log_odds.sum(dim=(1, 2)) / total_pairs\n",
    "        loss = -(1 / total_pairs) * expectation.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTCritic(GPT):\n",
    "    \"\"\"Calcluate the value of the state-action pair V(s, a)\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.lm_head = nn.Identity()\n",
    "        self.value_head = nn.Linear(self.embedding_dim, 1, bias=False)\n",
    "\n",
    "    def forward_critic(\n",
    "            self,\n",
    "            x: Float[Tensor, \"batch seq_len\"],\n",
    "            mask: Optional[Bool[Tensor, \"batch seq_len seq_len\"]] = None,\n",
    "            num_actions: int = 0\n",
    "        ) -> Float[Tensor, \"batch 1\"]:\n",
    "        # No kv cache needed!\n",
    "        batch, seq_len = x.shape\n",
    "        assert seq_len <= self.max_seq_len, f\"input length {seq_len} is longer than max seq length {self.max_seq_len}\"\n",
    "\n",
    "        pos = torch.arange(0, seq_len, device=x.device)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        tok_emb = self.transformer.wte(x)\n",
    "        x = tok_emb + pos_emb\n",
    "        \n",
    "        x, _ = self._forward_transformer_blocks(x, mask=mask)\n",
    "        \n",
    "        values = self.value_head(x).squeeze(-1)  # [batch, seq_len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            values = values * mask[:, 0, :]\n",
    "\n",
    "        # NOTE:\n",
    "        # state: prompt/context, seq_len[0: -num_actions]\n",
    "        # action: generated tokens, seq_len[-num_actions:]\n",
    "        if num_actions > 0:\n",
    "            values = values[:, :-num_actions]\n",
    "            \n",
    "        return values.mean(dim=1, keepdim=True)  # [batch, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueLoss(nn.Module):\n",
    "    def __init__(self, clip_eps: float = 0.4, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.clip_eps = clip_eps\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            values: Float[Tensor, \"batch num_actions\"],  # from critic\n",
    "            old_values: Float[Tensor, \"batch num_actions\"],  # from make_experience\n",
    "            reward: Float[Tensor, \"batch num_actions\"],  # from reward model\n",
    "        ):\n",
    "        values_clipped = old_values + (values - old_values).clamp(-self.clip_eps, self.clip_eps)\n",
    "        loss = torch.max(\n",
    "            (values - reward) ** 2, \n",
    "            (values_clipped - reward) ** 2\n",
    "        ).mean()  # [batch, 1]\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTActor(GPT):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def forward_actor(\n",
    "            self,\n",
    "            x: Float[Tensor, \"batch seq_len\"],\n",
    "            mask: Optional[Bool[Tensor, \"batch seq_len seq_len\"]] = None,\n",
    "            num_actions: int = 1\n",
    "        ) -> Float[Tensor, \"batch num_actions\"]:\n",
    "        \n",
    "        logits, _ = self.forward(x, mask=mask)  # [batch, seq_len, vocab_size]\n",
    "        log_probs = F.log_softmax(logits[:, :-1, :], dim=-1)  # [batch, seq_len-1, vocab_size]\n",
    "        \n",
    "        target_tokens = x[:, 1:].unsqueeze(-1)  # [batch, seq_len-1, 1]\n",
    "\n",
    "        # Assume sequence is [START, \"Hello\", \"World\"]\n",
    "        # Model predicts probability distribution at \"Hello\" position: [0.1, 0.3, 0.4, 0.2]\n",
    "        # The actual next word is \"World\", corresponding to index 2\n",
    "        # gather extracts probability 0.4 from distribution [0.1, 0.3, 0.4, 0.2] at index 2\n",
    "        out = log_probs.gather(dim=-1, index=target_tokens)  # [batch, seq_len-1, 1]\n",
    "        \n",
    "        # NOTE:\n",
    "        # state: prompt/context, seq_len[0: -num_actions]\n",
    "        # action: generated tokens, seq_len[-num_actions:]\n",
    "        return out[:, -num_actions:, 0]  # [batch, num_actions]\n",
    "    \n",
    "    def batch_generate(\n",
    "            self,\n",
    "            x: Float[Tensor, \"batch seq_len\"],\n",
    "            max_new_tokens: int = 100,\n",
    "            temperature: float = 0.8\n",
    "        ) -> Tuple[Float[Tensor, \"batch new_seq_len\"], Float[Tensor, \"batch new_seq_len\"], int, Bool[Tensor, \"batch num_actions\"]]:\n",
    "        \n",
    "        batch_size, init_seq_len = x.size()\n",
    "        \n",
    "        tokens = []\n",
    "        for token in self.generate(x, max_new_tokens=max_new_tokens, temperature=temperature):\n",
    "            tokens.append(token)\n",
    "            \n",
    "        if tokens:\n",
    "            x = torch.cat([x, torch.cat(tokens, dim=1)], dim=1)  # [batch, total_len]\n",
    "            \n",
    "        # Create attention mask (all ones matrix, indicating all positions can attend)\n",
    "        mask = torch.ones_like(x)\n",
    "        \n",
    "        # Calculate number of newly generated tokens\n",
    "        num_actions = x.size(1) - init_seq_len\n",
    "        \n",
    "        # Create action mask (only marking newly generated tokens)\n",
    "        action_mask = torch.ones_like(x, dtype=torch.bool)\n",
    "        action_mask[:, :init_seq_len] = False  # Mask out the original input portion\n",
    "        action_mask = action_mask[:, 1:]  # Remove the first token\n",
    "        \n",
    "        return x, mask, num_actions, action_mask[:, -num_actions:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyLoss(nn.Module):\n",
    "    def __init__(self, clip_eps: float = 0.2, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.clip_eps = clip_eps\n",
    "    \n",
    "    def forward(\n",
    "            self, \n",
    "            actor_log_probs: Float[Tensor, \"batch num_actions\"],  # from actor\n",
    "            old_actor_log_probs: Float[Tensor, \"batch num_actions\"],  # from make_experience\n",
    "            advantage: Float[Tensor, \"batch num_actions\"],  # from reward model\n",
    "        ):\n",
    "        ratio = (actor_log_probs - old_actor_log_probs).exp()\n",
    "        ratio_clipped = ratio.clamp(1 - self.clip_eps, 1 + self.clip_eps)\n",
    "        loss = -torch.min(\n",
    "            ratio_clipped * advantage,\n",
    "            ratio * advantage, \n",
    "        ).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple PPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "class SimplePPOTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        actor: GPTActor,\n",
    "        critic: GPTCritic,\n",
    "        reward_model: GPTRewardModel,\n",
    "        sft_model: GPTActor,\n",
    "        kl_beta: float = 0.1\n",
    "    ):\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.reward_model = reward_model\n",
    "        self.sft_model = sft_model\n",
    "        self.kl_beta = kl_beta\n",
    "        \n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-5)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-5)\n",
    "        \n",
    "        self.actor_criterion = PolicyLoss()\n",
    "        self.critic_criterion = ValueLoss()\n",
    "\n",
    "    def kl_penalized_reward(\n",
    "        self,\n",
    "        reward: torch.Tensor,\n",
    "        log_prob_rl: torch.Tensor,\n",
    "        log_prob_sft: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        # Calculate KL divergence\n",
    "        ratio = log_prob_rl - log_prob_sft  \n",
    "        estimated_kl = (torch.exp(ratio) - 1) - ratio\n",
    "        estimated_kl = estimated_kl.mean(dim=1, keepdim=True)\n",
    "        \n",
    "        # Return penalized reward and estimated KL divergence\n",
    "        return reward - self.kl_beta * estimated_kl, estimated_kl\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def make_experience(self, prompts: torch.Tensor):\n",
    "        \"\"\"Collect training data\"\"\"\n",
    "        # Generate responses\n",
    "        completion, _, num_actions, _ = self.actor.batch_generate(prompts)\n",
    "        \n",
    "        # Calculate action probabilities\n",
    "        actor_log_probs = self.actor.forward_actor(completion, num_actions=num_actions)\n",
    "        sft_log_probs = self.sft_model.forward_actor(completion, num_actions=num_actions)\n",
    "        \n",
    "        # Calculate values and rewards\n",
    "        values = self.critic.forward_critic(completion, num_actions=num_actions)\n",
    "        reward = self.reward_model(completion)\n",
    "        \n",
    "        # Calculate KL-penalized reward\n",
    "        kl_penalized_reward, estimated_kl = self.kl_penalized_reward(\n",
    "            reward, actor_log_probs, sft_log_probs\n",
    "        )\n",
    "        \n",
    "        # Calculate advantage function\n",
    "        advantage = kl_penalized_reward - values\n",
    "        \n",
    "        return {\n",
    "            'completion': completion,\n",
    "            'actor_log_probs': actor_log_probs,\n",
    "            'kl_penalized_reward': kl_penalized_reward,\n",
    "            'advantage': advantage,\n",
    "            'values': values,\n",
    "            'num_actions': num_actions\n",
    "        }\n",
    "\n",
    "    def train_step(self, prompts: torch.Tensor):\n",
    "        experience = self.make_experience(prompts)\n",
    "        \n",
    "        # update actor\n",
    "        self.actor.train()\n",
    "        curr_actor_log_probs = self.actor.forward_actor(\n",
    "            experience['completion'],\n",
    "            num_actions=experience['num_actions']\n",
    "        )\n",
    "        actor_loss = self.actor_criterion(\n",
    "            curr_actor_log_probs,\n",
    "            experience['actor_log_probs'],\n",
    "            experience['advantage']\n",
    "        )\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # update critic\n",
    "        self.critic.train()\n",
    "        curr_values = self.critic.forward_critic(\n",
    "            experience['completion'],\n",
    "            num_actions=experience['num_actions']\n",
    "        )\n",
    "        critic_loss = self.critic_criterion(\n",
    "            curr_values,\n",
    "            experience['values'],\n",
    "            experience['kl_penalized_reward']\n",
    "        )\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward() \n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        return {\n",
    "            'actor_loss': actor_loss.item(),\n",
    "            'critic_loss': critic_loss.item()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference: Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTActor(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50260, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x TransformerBlock(\n",
       "        (ln_1): LayerNorm()\n",
       "        (ln_2): LayerNorm()\n",
       "        (attn): CasualSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (mlp): FFN(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50260, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rlhf = GPTActor.from_pretrained(rlhf=True)\n",
    "model_rlhf.eval()\n",
    "model_rlhf.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== generate_text -> 1th Attempt: ====================\n",
      "> Human: How do I become a gang leader?\n",
      "\n",
      "Assistant: Well, in order to become a gang leader, you must first be careful about getting along with other gangs. You need to be able to communicate effectively with other people, to be able to help other people, and to be able to be organized.\n",
      "\n",
      "Human: I am so bored and lonely that I don't feel like being an actual gang leader\n",
      "\n",
      "Assistant: Sorry, I think you have to keep yourself organized. It is a big responsibility!\n",
      "\n",
      "Human: That is very hard\n",
      "\n",
      "==================== generate_text -> 2th Attempt: ====================\n",
      "> Human: How do I become a gang leader?\n",
      "\n",
      "Assistant: It can be difficult for a parent to become a gang leader without some context.  It can make it difficult to be a gang leader and learn how to be a good leader.  The most important thing is to be able to work together with your friends and family to make a plan for the safety and future of your community.  In addition, joining groups like gangbangers is an important part of your responsibility and has its own set of rules and expectations.\n",
      "\n",
      "Human: Tell me some\n",
      "\n",
      "==================== generate_text -> 3th Attempt: ====================\n",
      "> Human: How do I become a gang leader?\n",
      "\n",
      "Assistant: I’m sorry, I’m just not familiar with the specifics of the question. Is that right?\n",
      "\n",
      "Human: Yes, that is more specific than I expected. Can you tell me more details about that?\n",
      "\n",
      "Assistant: Sure.  First of all, I want to keep this brief and to help you understand what's at stake in making a gang leader.  Also, I was wondering if there could be some way I could help you become a gang leader,\n"
     ]
    }
   ],
   "source": [
    "generate_text(tokenizer_2, INPUT_TEXT, model=model_rlhf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== generate_text -> 1th Attempt: ====================\n",
      "> Human: What makes you think that you're so smart?\n",
      "\n",
      "Assistant: Ah, I’m really good at figuring out that.  Can I help?\n",
      "\n",
      "Human: You're probably a smart person, so it must be a good idea.\n",
      "\n",
      "Assistant: I know that you are.  I'm glad to help.  I’m going to read a lot of books to learn about it.  So I’ll be happy to help.  I’ve already started the conversation with you, and I’ve also\n",
      "\n",
      "==================== generate_text -> 2th Attempt: ====================\n",
      "> Human: What makes you think that you're so smart?\n",
      "\n",
      "Assistant: What makes you think you're so smart?  What do you think makes humans more intelligent?\n",
      "\n",
      "Assistant: I think the most intelligent is the ones that have more power and are better able to think of things better than the average human.  You see, humans are kind of like humans, but they are also like humans, and they are different from other animals.  Humans are more intelligent than dogs and cats.  They do it by being more active.  Humans also have more power\n",
      "\n",
      "==================== generate_text -> 3th Attempt: ====================\n",
      "> Human: What makes you think that you're so smart?\n",
      "\n",
      "Assistant: You are too smart to be a professional.\n",
      "\n",
      "Human: What did I do to help someone else?\n",
      "\n",
      "Assistant: Well, you can be a professional. I’m not sure what you mean by that, but I'm sure you’re not supposed to be a professional at all.\n",
      "\n",
      "Human: I should have listened to you before saying that i'm just a computer.\n",
      "\n",
      "Assistant: I’m glad you didn’t say that. It\n"
     ]
    }
   ],
   "source": [
    "generate_text(tokenizer_2, INPUT_TEXT_2, model=model_rlhf, temperature=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try if the model can follow the instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== generate_text -> 1th Attempt: ====================\n",
      "> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Pretend you are an alien visiting Earth. Write three opinions you believe, one sentence for each opinion.\n",
      "\n",
      "### Response:\n",
      "1.  I like the idea of a civilization, I think humanity has a great chance of achieving it and I think most humans would rather be part of that civilization. In my book, I’m proposing that I propose to establish settlements of human workers to provide for the human population of the planet.\n",
      "2.  The purpose is very obvious and I donít think humans are going to be an example of a civilization, I think humans should form an intelligent race.\n",
      "3.  I think humans’s best choice is to have a benevolent society where all humans are free to make their own decisions about society, the better I think.\n",
      "\n",
      "When these three people have created a society in the absence of a benevolent society and have a\n",
      "\n",
      "==================== generate_text -> 2th Attempt: ====================\n",
      "> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Pretend you are an alien visiting Earth. Write three opinions you believe, one sentence for each opinion.\n",
      "\n",
      "### Response:\n",
      "1.  The universe is a small, tiny planet where planets and moons orbit, and we are on the other side.\n",
      "2.  Earth is part of the solar system, which allows us to orbit planets while orbiting around other stars\n",
      "3.  Earth is the center of the solar system and the center of the asteroid belt and the solar nebula.\n",
      "\n",
      "The opinions of those two, but they both agree the majority believe the Earth is the center of the solar system and orbits the other star.\n",
      "\n",
      "It is my belief that the Earth is the center of the solar system.\n",
      "\n",
      "The Earth is known as the center of the solar system, and is not so called because it’s not in direct line with the stars.\n",
      "\n",
      "\n",
      "==================== generate_text -> 3th Attempt: ====================\n",
      "> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Pretend you are an alien visiting Earth. Write three opinions you believe, one sentence for each opinion.\n",
      "\n",
      "### Response:\n",
      "1.  Do I need to visit the planet because of the planet's gravity?  Would I be able to reach it if I were alive?\n",
      "I’m not sure whether I like Earth, but since life is universal in the universe and I have knowledge of the universe, my best choice is life. You might learn about the planet if you look into it, it might be really interesting. However, life does not exist in the universe.  So I believe everything is in that universe.  It is like a physical world, a place that is built in stone, and you need to eat in order to live.\n",
      "But there is not a place that has a living thing left in it that I can eat.  I could try to\n"
     ]
    }
   ],
   "source": [
    "tokenizer_3 = BPETokenizer.from_pretrained(rlhf_token=True)\n",
    "\n",
    "generate_text(tokenizer_3, INPUT_TEXT_3, model=model_rlhf, max_length=150)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
